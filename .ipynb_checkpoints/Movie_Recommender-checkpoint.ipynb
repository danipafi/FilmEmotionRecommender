{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7327807b",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785ec29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c7fea",
   "metadata": {},
   "source": [
    "# CSVs Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f414f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame\n",
    "reviews = pd.read_csv('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/reviews_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de94e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>creationDate</th>\n",
       "      <th>criticName</th>\n",
       "      <th>isTopCritic</th>\n",
       "      <th>reviewState</th>\n",
       "      <th>publicatioName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>scoreSentiment</th>\n",
       "      <th>cleanedReviewText</th>\n",
       "      <th>predicted_moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>[('excitement', 35.543839830098335), ('approva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_mask</td>\n",
       "      <td>Blood Mask</td>\n",
       "      <td>1636744</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>The Foywonder</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Dread Central</td>\n",
       "      <td>It doesn't matter if a movie costs 300 million...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>It doesnt matter if a movie costs 300 million ...</td>\n",
       "      <td>[('disapproval', 56.05641547237329), ('annoyan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>city_hunter_shinjuku_private_eyes</td>\n",
       "      <td>City Hunter: Shinjuku Private Eyes</td>\n",
       "      <td>2590987</td>\n",
       "      <td>2019-05-28</td>\n",
       "      <td>Reuben Baron</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>CBR</td>\n",
       "      <td>The choreography is so precise and lifelike at...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>The choreography is so precise and lifelike at...</td>\n",
       "      <td>[('admiration', 80.08288365216127), ('approval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city_hunter_shinjuku_private_eyes</td>\n",
       "      <td>City Hunter: Shinjuku Private Eyes</td>\n",
       "      <td>2558908</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>Matt Schley</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Japan Times</td>\n",
       "      <td>The film's out-of-touch attempts at humor may ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>The films outoftouch attempts at humor may fin...</td>\n",
       "      <td>[('amusement', 22.690977576957067), ('realizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dangerous_men_2015</td>\n",
       "      <td>Dangerous Men</td>\n",
       "      <td>2504681</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>Pat Padua</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>DCist</td>\n",
       "      <td>Its clumsy determination is endearing and some...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its clumsy determination is endearing and some...</td>\n",
       "      <td>[('amusement', 49.197768434566136), ('admirati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id                               title  \\\n",
       "0                            beavers                             Beavers   \n",
       "1                         blood_mask                          Blood Mask   \n",
       "2  city_hunter_shinjuku_private_eyes  City Hunter: Shinjuku Private Eyes   \n",
       "3  city_hunter_shinjuku_private_eyes  City Hunter: Shinjuku Private Eyes   \n",
       "4                 dangerous_men_2015                       Dangerous Men   \n",
       "\n",
       "   reviewId creationDate       criticName  isTopCritic reviewState  \\\n",
       "0   1145982   2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "1   1636744   2007-06-02    The Foywonder        False      rotten   \n",
       "2   2590987   2019-05-28     Reuben Baron        False       fresh   \n",
       "3   2558908   2019-02-14      Matt Schley        False      rotten   \n",
       "4   2504681   2018-08-29        Pat Padua        False       fresh   \n",
       "\n",
       "                  publicatioName  \\\n",
       "0  Deseret News (Salt Lake City)   \n",
       "1                  Dread Central   \n",
       "2                            CBR   \n",
       "3                    Japan Times   \n",
       "4                          DCist   \n",
       "\n",
       "                                          reviewText scoreSentiment  \\\n",
       "0  Timed to be just long enough for most youngste...       POSITIVE   \n",
       "1  It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
       "2  The choreography is so precise and lifelike at...       POSITIVE   \n",
       "3  The film's out-of-touch attempts at humor may ...       NEGATIVE   \n",
       "4  Its clumsy determination is endearing and some...       POSITIVE   \n",
       "\n",
       "                                   cleanedReviewText  \\\n",
       "0  Timed to be just long enough for most youngste...   \n",
       "1  It doesnt matter if a movie costs 300 million ...   \n",
       "2  The choreography is so precise and lifelike at...   \n",
       "3  The films outoftouch attempts at humor may fin...   \n",
       "4  Its clumsy determination is endearing and some...   \n",
       "\n",
       "                                     predicted_moods  \n",
       "0  [('excitement', 35.543839830098335), ('approva...  \n",
       "1  [('disapproval', 56.05641547237329), ('annoyan...  \n",
       "2  [('admiration', 80.08288365216127), ('approval...  \n",
       "3  [('amusement', 22.690977576957067), ('realizat...  \n",
       "4  [('amusement', 49.197768434566136), ('admirati...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62860bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/clean_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "471849d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genre</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love_lies</td>\n",
       "      <td>Love, Lies</td>\n",
       "      <td>43.0</td>\n",
       "      <td>65.76</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Park Heung-Sik,Heung-Sik Park</td>\n",
       "      <td>Ha Young-Joon,Jeon Yun-su,Song Hye-jin</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adrift_2018</td>\n",
       "      <td>Adrift</td>\n",
       "      <td>65.0</td>\n",
       "      <td>69.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>English</td>\n",
       "      <td>Baltasar Kormákur</td>\n",
       "      <td>Aaron Kandell,Jordan Kandell,David Branson Smith</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adrift_2018</td>\n",
       "      <td>Adrift</td>\n",
       "      <td>65.0</td>\n",
       "      <td>69.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Drama</td>\n",
       "      <td>English</td>\n",
       "      <td>Baltasar Kormákur</td>\n",
       "      <td>Aaron Kandell,Jordan Kandell,David Branson Smith</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adrift_2018</td>\n",
       "      <td>Adrift</td>\n",
       "      <td>65.0</td>\n",
       "      <td>69.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Romance</td>\n",
       "      <td>English</td>\n",
       "      <td>Baltasar Kormákur</td>\n",
       "      <td>Aaron Kandell,Jordan Kandell,David Branson Smith</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035316-born_to_kill</td>\n",
       "      <td>Born to Kill</td>\n",
       "      <td>74.0</td>\n",
       "      <td>83.00</td>\n",
       "      <td>92.0</td>\n",
       "      <td>Crime</td>\n",
       "      <td>English</td>\n",
       "      <td>Robert Wise</td>\n",
       "      <td>Eve Greene,Richard Macaulay</td>\n",
       "      <td>1947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id         title  audienceScore  tomatoMeter  \\\n",
       "0             love_lies    Love, Lies           43.0        65.76   \n",
       "1           adrift_2018        Adrift           65.0        69.00   \n",
       "2           adrift_2018        Adrift           65.0        69.00   \n",
       "3           adrift_2018        Adrift           65.0        69.00   \n",
       "4  1035316-born_to_kill  Born to Kill           74.0        83.00   \n",
       "\n",
       "   runtimeMinutes      genre originalLanguage                       director  \\\n",
       "0           120.0      Drama           Korean  Park Heung-Sik,Heung-Sik Park   \n",
       "1           120.0  Adventure          English              Baltasar Kormákur   \n",
       "2           120.0      Drama          English              Baltasar Kormákur   \n",
       "3           120.0    Romance          English              Baltasar Kormákur   \n",
       "4            92.0      Crime          English                    Robert Wise   \n",
       "\n",
       "                                             writer release_year  \n",
       "0            Ha Young-Joon,Jeon Yun-su,Song Hye-jin      Unknown  \n",
       "1  Aaron Kandell,Jordan Kandell,David Branson Smith         2018  \n",
       "2  Aaron Kandell,Jordan Kandell,David Branson Smith         2018  \n",
       "3  Aaron Kandell,Jordan Kandell,David Branson Smith         2018  \n",
       "4                       Eve Greene,Richard Macaulay         1947  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d56e0a",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462da63f",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7cef3",
   "metadata": {},
   "source": [
    "Group by id and transform the genres into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1bff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_grouped = movies.groupby('id')['genre'].apply(list).reset_index()\n",
    "\n",
    "# Merge the result back into the original DataFrame, keeping all columns\n",
    "movies = movies.drop(columns=['genre']).drop_duplicates(subset=['id']).merge(movies_grouped, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400389c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love_lies</td>\n",
       "      <td>Love, Lies</td>\n",
       "      <td>43.00</td>\n",
       "      <td>65.76</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Park Heung-Sik,Heung-Sik Park</td>\n",
       "      <td>Ha Young-Joon,Jeon Yun-su,Song Hye-jin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adrift_2018</td>\n",
       "      <td>Adrift</td>\n",
       "      <td>65.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Baltasar Kormákur</td>\n",
       "      <td>Aaron Kandell,Jordan Kandell,David Branson Smith</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Adventure, Drama, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035316-born_to_kill</td>\n",
       "      <td>Born to Kill</td>\n",
       "      <td>74.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>92.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Robert Wise</td>\n",
       "      <td>Eve Greene,Richard Macaulay</td>\n",
       "      <td>1947</td>\n",
       "      <td>[Crime, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garden_murder_case</td>\n",
       "      <td>The Garden Murder Case</td>\n",
       "      <td>55.67</td>\n",
       "      <td>65.76</td>\n",
       "      <td>61.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Edwin L. Marin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2016</td>\n",
       "      <td>[Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>margarita_happy_hour</td>\n",
       "      <td>Margarita Happy Hour</td>\n",
       "      <td>55.67</td>\n",
       "      <td>76.00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Ilya Chaiken</td>\n",
       "      <td>Ilya Chaiken</td>\n",
       "      <td>2002</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                   title  audienceScore  tomatoMeter  \\\n",
       "0             love_lies              Love, Lies          43.00        65.76   \n",
       "1           adrift_2018                  Adrift          65.00        69.00   \n",
       "2  1035316-born_to_kill            Born to Kill          74.00        83.00   \n",
       "3    garden_murder_case  The Garden Murder Case          55.67        65.76   \n",
       "4  margarita_happy_hour    Margarita Happy Hour          55.67        76.00   \n",
       "\n",
       "   runtimeMinutes originalLanguage                       director  \\\n",
       "0           120.0           Korean  Park Heung-Sik,Heung-Sik Park   \n",
       "1           120.0          English              Baltasar Kormákur   \n",
       "2            92.0          English                    Robert Wise   \n",
       "3            61.0          English                 Edwin L. Marin   \n",
       "4            98.0          English                   Ilya Chaiken   \n",
       "\n",
       "                                             writer release_year  \\\n",
       "0            Ha Young-Joon,Jeon Yun-su,Song Hye-jin      Unknown   \n",
       "1  Aaron Kandell,Jordan Kandell,David Branson Smith         2018   \n",
       "2                       Eve Greene,Richard Macaulay         1947   \n",
       "3                                           Unknown         2016   \n",
       "4                                      Ilya Chaiken         2002   \n",
       "\n",
       "                         genre  \n",
       "0                      [Drama]  \n",
       "1  [Adventure, Drama, Romance]  \n",
       "2               [Crime, Drama]  \n",
       "3                   [Thriller]  \n",
       "4                      [Drama]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776e4725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13046</th>\n",
       "      <td>how_it_ends_2021</td>\n",
       "      <td>How It Ends</td>\n",
       "      <td>42.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Zoe Lister Jones,Daryl Wein</td>\n",
       "      <td>Daryl Wein,Zoe Lister Jones</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47022</th>\n",
       "      <td>how_it_ends</td>\n",
       "      <td>How It Ends</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>English</td>\n",
       "      <td>David M. Rosenthal</td>\n",
       "      <td>Brooks McLaren</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Sci-fi, Thriller, Action, Adventure]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        title  audienceScore  tomatoMeter  \\\n",
       "13046  how_it_ends_2021  How It Ends           42.0         68.0   \n",
       "47022       how_it_ends  How It Ends           17.0         17.0   \n",
       "\n",
       "       runtimeMinutes originalLanguage                     director  \\\n",
       "13046            82.0          English  Zoe Lister Jones,Daryl Wein   \n",
       "47022           113.0          English           David M. Rosenthal   \n",
       "\n",
       "                            writer release_year  \\\n",
       "13046  Daryl Wein,Zoe Lister Jones         2021   \n",
       "47022               Brooks McLaren         2018   \n",
       "\n",
       "                                       genre  \n",
       "13046                        [Comedy, Drama]  \n",
       "47022  [Sci-fi, Thriller, Action, Adventure]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.loc[movies['title'] == 'How It Ends']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273def56",
   "metadata": {},
   "source": [
    "### Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fefb5",
   "metadata": {},
   "source": [
    "#### Calculating the average emotions per movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e81b87",
   "metadata": {},
   "source": [
    "#### Explode and split the data\n",
    "First, explode and split the predicted_moods column into emotion and probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538cefcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>creationDate</th>\n",
       "      <th>criticName</th>\n",
       "      <th>isTopCritic</th>\n",
       "      <th>reviewState</th>\n",
       "      <th>publicatioName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>scoreSentiment</th>\n",
       "      <th>cleanedReviewText</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>excitement</td>\n",
       "      <td>35.543840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>approval</td>\n",
       "      <td>33.460199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>admiration</td>\n",
       "      <td>14.496543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_mask</td>\n",
       "      <td>Blood Mask</td>\n",
       "      <td>1636744</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>The Foywonder</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Dread Central</td>\n",
       "      <td>It doesn't matter if a movie costs 300 million...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>It doesnt matter if a movie costs 300 million ...</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>56.056415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_mask</td>\n",
       "      <td>Blood Mask</td>\n",
       "      <td>1636744</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>The Foywonder</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Dread Central</td>\n",
       "      <td>It doesn't matter if a movie costs 300 million...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>It doesnt matter if a movie costs 300 million ...</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>16.554233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363577</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706148</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Jake Cole</td>\n",
       "      <td>True</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>Across Taika Waititi&amp;#8217;s film&amp;#44; a war a...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Across Taika Waititis film a war against the g...</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>29.755378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363577</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706148</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Jake Cole</td>\n",
       "      <td>True</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>Across Taika Waititi&amp;#8217;s film&amp;#44; a war a...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Across Taika Waititis film a war against the g...</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>16.977576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363578</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706147</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Movie Nation</td>\n",
       "      <td>It&amp;#8217;s the jokes that make it&amp;#44; with th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its the jokes that make it with the selfmockin...</td>\n",
       "      <td>amusement</td>\n",
       "      <td>66.380384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363578</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706147</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Movie Nation</td>\n",
       "      <td>It&amp;#8217;s the jokes that make it&amp;#44; with th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its the jokes that make it with the selfmockin...</td>\n",
       "      <td>joy</td>\n",
       "      <td>22.703058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363578</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706147</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Movie Nation</td>\n",
       "      <td>It&amp;#8217;s the jokes that make it&amp;#44; with th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its the jokes that make it with the selfmockin...</td>\n",
       "      <td>approval</td>\n",
       "      <td>3.990644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4090737 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id                   title   reviewId  \\\n",
       "0                      beavers                 Beavers    1145982   \n",
       "0                      beavers                 Beavers    1145982   \n",
       "0                      beavers                 Beavers    1145982   \n",
       "1                   blood_mask              Blood Mask    1636744   \n",
       "1                   blood_mask              Blood Mask    1636744   \n",
       "...                        ...                     ...        ...   \n",
       "1363577  thor_love_and_thunder  Thor: Love and Thunder  102706148   \n",
       "1363577  thor_love_and_thunder  Thor: Love and Thunder  102706148   \n",
       "1363578  thor_love_and_thunder  Thor: Love and Thunder  102706147   \n",
       "1363578  thor_love_and_thunder  Thor: Love and Thunder  102706147   \n",
       "1363578  thor_love_and_thunder  Thor: Love and Thunder  102706147   \n",
       "\n",
       "        creationDate       criticName  isTopCritic reviewState  \\\n",
       "0         2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "0         2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "0         2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "1         2007-06-02    The Foywonder        False      rotten   \n",
       "1         2007-06-02    The Foywonder        False      rotten   \n",
       "...              ...              ...          ...         ...   \n",
       "1363577   2022-07-05        Jake Cole         True      rotten   \n",
       "1363577   2022-07-05        Jake Cole         True      rotten   \n",
       "1363578   2022-07-05      Roger Moore        False       fresh   \n",
       "1363578   2022-07-05      Roger Moore        False       fresh   \n",
       "1363578   2022-07-05      Roger Moore        False       fresh   \n",
       "\n",
       "                        publicatioName  \\\n",
       "0        Deseret News (Salt Lake City)   \n",
       "0        Deseret News (Salt Lake City)   \n",
       "0        Deseret News (Salt Lake City)   \n",
       "1                        Dread Central   \n",
       "1                        Dread Central   \n",
       "...                                ...   \n",
       "1363577                 Slant Magazine   \n",
       "1363577                 Slant Magazine   \n",
       "1363578                   Movie Nation   \n",
       "1363578                   Movie Nation   \n",
       "1363578                   Movie Nation   \n",
       "\n",
       "                                                reviewText scoreSentiment  \\\n",
       "0        Timed to be just long enough for most youngste...       POSITIVE   \n",
       "0        Timed to be just long enough for most youngste...       POSITIVE   \n",
       "0        Timed to be just long enough for most youngste...       POSITIVE   \n",
       "1        It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
       "1        It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
       "...                                                    ...            ...   \n",
       "1363577  Across Taika Waititi&#8217;s film&#44; a war a...       NEGATIVE   \n",
       "1363577  Across Taika Waititi&#8217;s film&#44; a war a...       NEGATIVE   \n",
       "1363578  It&#8217;s the jokes that make it&#44; with th...       POSITIVE   \n",
       "1363578  It&#8217;s the jokes that make it&#44; with th...       POSITIVE   \n",
       "1363578  It&#8217;s the jokes that make it&#44; with th...       POSITIVE   \n",
       "\n",
       "                                         cleanedReviewText      emotion  \\\n",
       "0        Timed to be just long enough for most youngste...   excitement   \n",
       "0        Timed to be just long enough for most youngste...     approval   \n",
       "0        Timed to be just long enough for most youngste...   admiration   \n",
       "1        It doesnt matter if a movie costs 300 million ...  disapproval   \n",
       "1        It doesnt matter if a movie costs 300 million ...    annoyance   \n",
       "...                                                    ...          ...   \n",
       "1363577  Across Taika Waititis film a war against the g...    annoyance   \n",
       "1363577  Across Taika Waititis film a war against the g...  disapproval   \n",
       "1363578  Its the jokes that make it with the selfmockin...    amusement   \n",
       "1363578  Its the jokes that make it with the selfmockin...          joy   \n",
       "1363578  Its the jokes that make it with the selfmockin...     approval   \n",
       "\n",
       "         probability  \n",
       "0          35.543840  \n",
       "0          33.460199  \n",
       "0          14.496543  \n",
       "1          56.056415  \n",
       "1          16.554233  \n",
       "...              ...  \n",
       "1363577    29.755378  \n",
       "1363577    16.977576  \n",
       "1363578    66.380384  \n",
       "1363578    22.703058  \n",
       "1363578     3.990644  \n",
       "\n",
       "[4090737 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tuple strings into actual tuples\n",
    "reviews['predicted_moods'] = reviews['predicted_moods'].apply(ast.literal_eval)\n",
    "\n",
    "# Explode the column\n",
    "reviews_exploded = reviews.explode('predicted_moods')\n",
    "\n",
    "# Split the tuples into separate columns\n",
    "reviews_exploded[['emotion', 'probability']] = pd.DataFrame(reviews_exploded['predicted_moods'].tolist(), index=reviews_exploded.index)\n",
    "reviews_exploded = reviews_exploded.drop(columns=['predicted_moods'])\n",
    "\n",
    "# Convert 'probability' to numeric\n",
    "reviews_exploded['probability'] = pd.to_numeric(reviews_exploded['probability'])\n",
    "\n",
    "# Verify the result\n",
    "reviews_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a20551",
   "metadata": {},
   "source": [
    "#### Normalize probabilities to sum to 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99bb28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>creationDate</th>\n",
       "      <th>criticName</th>\n",
       "      <th>isTopCritic</th>\n",
       "      <th>reviewState</th>\n",
       "      <th>publicatioName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>scoreSentiment</th>\n",
       "      <th>cleanedReviewText</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability</th>\n",
       "      <th>probability_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>excitement</td>\n",
       "      <td>35.543840</td>\n",
       "      <td>42.5672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>approval</td>\n",
       "      <td>33.460199</td>\n",
       "      <td>40.0718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavers</td>\n",
       "      <td>Beavers</td>\n",
       "      <td>1145982</td>\n",
       "      <td>2003-05-23</td>\n",
       "      <td>Ivan M. Lincoln</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Deseret News (Salt Lake City)</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Timed to be just long enough for most youngste...</td>\n",
       "      <td>admiration</td>\n",
       "      <td>14.496543</td>\n",
       "      <td>17.3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_mask</td>\n",
       "      <td>Blood Mask</td>\n",
       "      <td>1636744</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>The Foywonder</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Dread Central</td>\n",
       "      <td>It doesn't matter if a movie costs 300 million...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>It doesnt matter if a movie costs 300 million ...</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>56.056415</td>\n",
       "      <td>70.6446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_mask</td>\n",
       "      <td>Blood Mask</td>\n",
       "      <td>1636744</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>The Foywonder</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Dread Central</td>\n",
       "      <td>It doesn't matter if a movie costs 300 million...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>It doesnt matter if a movie costs 300 million ...</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>16.554233</td>\n",
       "      <td>20.8623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363577</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706148</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Jake Cole</td>\n",
       "      <td>True</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>Across Taika Waititi&amp;#8217;s film&amp;#44; a war a...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Across Taika Waititis film a war against the g...</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>29.755378</td>\n",
       "      <td>37.8302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363577</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706148</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Jake Cole</td>\n",
       "      <td>True</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>Across Taika Waititi&amp;#8217;s film&amp;#44; a war a...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Across Taika Waititis film a war against the g...</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>16.977576</td>\n",
       "      <td>21.5848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363578</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706147</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Movie Nation</td>\n",
       "      <td>It&amp;#8217;s the jokes that make it&amp;#44; with th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its the jokes that make it with the selfmockin...</td>\n",
       "      <td>amusement</td>\n",
       "      <td>66.380384</td>\n",
       "      <td>71.3199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363578</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706147</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Movie Nation</td>\n",
       "      <td>It&amp;#8217;s the jokes that make it&amp;#44; with th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its the jokes that make it with the selfmockin...</td>\n",
       "      <td>joy</td>\n",
       "      <td>22.703058</td>\n",
       "      <td>24.3925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363578</th>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>102706147</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Movie Nation</td>\n",
       "      <td>It&amp;#8217;s the jokes that make it&amp;#44; with th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>Its the jokes that make it with the selfmockin...</td>\n",
       "      <td>approval</td>\n",
       "      <td>3.990644</td>\n",
       "      <td>4.2876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4090737 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id                   title   reviewId  \\\n",
       "0                      beavers                 Beavers    1145982   \n",
       "0                      beavers                 Beavers    1145982   \n",
       "0                      beavers                 Beavers    1145982   \n",
       "1                   blood_mask              Blood Mask    1636744   \n",
       "1                   blood_mask              Blood Mask    1636744   \n",
       "...                        ...                     ...        ...   \n",
       "1363577  thor_love_and_thunder  Thor: Love and Thunder  102706148   \n",
       "1363577  thor_love_and_thunder  Thor: Love and Thunder  102706148   \n",
       "1363578  thor_love_and_thunder  Thor: Love and Thunder  102706147   \n",
       "1363578  thor_love_and_thunder  Thor: Love and Thunder  102706147   \n",
       "1363578  thor_love_and_thunder  Thor: Love and Thunder  102706147   \n",
       "\n",
       "        creationDate       criticName  isTopCritic reviewState  \\\n",
       "0         2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "0         2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "0         2003-05-23  Ivan M. Lincoln        False       fresh   \n",
       "1         2007-06-02    The Foywonder        False      rotten   \n",
       "1         2007-06-02    The Foywonder        False      rotten   \n",
       "...              ...              ...          ...         ...   \n",
       "1363577   2022-07-05        Jake Cole         True      rotten   \n",
       "1363577   2022-07-05        Jake Cole         True      rotten   \n",
       "1363578   2022-07-05      Roger Moore        False       fresh   \n",
       "1363578   2022-07-05      Roger Moore        False       fresh   \n",
       "1363578   2022-07-05      Roger Moore        False       fresh   \n",
       "\n",
       "                        publicatioName  \\\n",
       "0        Deseret News (Salt Lake City)   \n",
       "0        Deseret News (Salt Lake City)   \n",
       "0        Deseret News (Salt Lake City)   \n",
       "1                        Dread Central   \n",
       "1                        Dread Central   \n",
       "...                                ...   \n",
       "1363577                 Slant Magazine   \n",
       "1363577                 Slant Magazine   \n",
       "1363578                   Movie Nation   \n",
       "1363578                   Movie Nation   \n",
       "1363578                   Movie Nation   \n",
       "\n",
       "                                                reviewText scoreSentiment  \\\n",
       "0        Timed to be just long enough for most youngste...       POSITIVE   \n",
       "0        Timed to be just long enough for most youngste...       POSITIVE   \n",
       "0        Timed to be just long enough for most youngste...       POSITIVE   \n",
       "1        It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
       "1        It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
       "...                                                    ...            ...   \n",
       "1363577  Across Taika Waititi&#8217;s film&#44; a war a...       NEGATIVE   \n",
       "1363577  Across Taika Waititi&#8217;s film&#44; a war a...       NEGATIVE   \n",
       "1363578  It&#8217;s the jokes that make it&#44; with th...       POSITIVE   \n",
       "1363578  It&#8217;s the jokes that make it&#44; with th...       POSITIVE   \n",
       "1363578  It&#8217;s the jokes that make it&#44; with th...       POSITIVE   \n",
       "\n",
       "                                         cleanedReviewText      emotion  \\\n",
       "0        Timed to be just long enough for most youngste...   excitement   \n",
       "0        Timed to be just long enough for most youngste...     approval   \n",
       "0        Timed to be just long enough for most youngste...   admiration   \n",
       "1        It doesnt matter if a movie costs 300 million ...  disapproval   \n",
       "1        It doesnt matter if a movie costs 300 million ...    annoyance   \n",
       "...                                                    ...          ...   \n",
       "1363577  Across Taika Waititis film a war against the g...    annoyance   \n",
       "1363577  Across Taika Waititis film a war against the g...  disapproval   \n",
       "1363578  Its the jokes that make it with the selfmockin...    amusement   \n",
       "1363578  Its the jokes that make it with the selfmockin...          joy   \n",
       "1363578  Its the jokes that make it with the selfmockin...     approval   \n",
       "\n",
       "         probability  probability_normalized  \n",
       "0          35.543840                 42.5672  \n",
       "0          33.460199                 40.0718  \n",
       "0          14.496543                 17.3610  \n",
       "1          56.056415                 70.6446  \n",
       "1          16.554233                 20.8623  \n",
       "...              ...                     ...  \n",
       "1363577    29.755378                 37.8302  \n",
       "1363577    16.977576                 21.5848  \n",
       "1363578    66.380384                 71.3199  \n",
       "1363578    22.703058                 24.3925  \n",
       "1363578     3.990644                  4.2876  \n",
       "\n",
       "[4090737 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize probabilities to sum to 100%\n",
    "sum_probabilities = reviews_exploded.groupby('reviewId')['probability'].transform('sum')\n",
    "reviews_exploded['probability_normalized'] = (reviews_exploded['probability'] / sum_probabilities) * 100\n",
    "\n",
    "# Round the normalized probabilities to 4 decimal places\n",
    "reviews_exploded['probability_normalized'] = reviews_exploded['probability_normalized'].round(4)\n",
    "\n",
    "# Verify the result\n",
    "reviews_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4b4a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.]\n"
     ]
    }
   ],
   "source": [
    "# Verify the sum of normalized probabilities\n",
    "sum_normalized = reviews_exploded.groupby('reviewId')['probability_normalized'].sum()\n",
    "\n",
    "# Round the sums to 2 decimal places for display\n",
    "sum_normalized = sum_normalized.round(2)\n",
    "\n",
    "# Check unique values\n",
    "print(sum_normalized.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd01b34",
   "metadata": {},
   "source": [
    "#### Group by id and emotion and calculate the mean probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f25ba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>admiration</td>\n",
       "      <td>43.15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>amusement</td>\n",
       "      <td>33.83000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>36.67500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>approval</td>\n",
       "      <td>34.83245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>disappointment</td>\n",
       "      <td>32.67010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>50.40925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>joy</td>\n",
       "      <td>21.32340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>optimism</td>\n",
       "      <td>11.22480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>realization</td>\n",
       "      <td>7.48920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>admiration</td>\n",
       "      <td>49.29476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id         emotion  probability_normalized\n",
       "0       $5_a_day      admiration                43.15200\n",
       "1       $5_a_day       amusement                33.83000\n",
       "2       $5_a_day       annoyance                36.67500\n",
       "3       $5_a_day        approval                34.83245\n",
       "4       $5_a_day  disappointment                32.67010\n",
       "5       $5_a_day     disapproval                50.40925\n",
       "6       $5_a_day             joy                21.32340\n",
       "7       $5_a_day        optimism                11.22480\n",
       "8       $5_a_day     realization                 7.48920\n",
       "9  009_re_cyborg      admiration                49.29476"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_grouped = reviews_exploded.groupby(['id', 'emotion'])['probability_normalized'].mean().reset_index()\n",
    "\n",
    "# Verify the result\n",
    "reviews_grouped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc880567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>creationDate</th>\n",
       "      <th>criticName</th>\n",
       "      <th>isTopCritic</th>\n",
       "      <th>reviewState</th>\n",
       "      <th>publicatioName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>scoreSentiment</th>\n",
       "      <th>cleanedReviewText</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability</th>\n",
       "      <th>probability_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1213414</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>2097498</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>Kevin Carr</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>7M Pictures</td>\n",
       "      <td>$5 a Day isn't perfect, but it does examine some of the issues that we have when connecting with our parents as adults.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>5 a Day isnt perfect but it does examine some of the issues that we have when connecting with our parents as adults</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>60.371897</td>\n",
       "      <td>70.1637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213414</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>2097498</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>Kevin Carr</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>7M Pictures</td>\n",
       "      <td>$5 a Day isn't perfect, but it does examine some of the issues that we have when connecting with our parents as adults.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>5 a Day isnt perfect but it does examine some of the issues that we have when connecting with our parents as adults</td>\n",
       "      <td>approval</td>\n",
       "      <td>19.228371</td>\n",
       "      <td>22.3471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213414</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>2097498</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>Kevin Carr</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>7M Pictures</td>\n",
       "      <td>$5 a Day isn't perfect, but it does examine some of the issues that we have when connecting with our parents as adults.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>5 a Day isnt perfect but it does examine some of the issues that we have when connecting with our parents as adults</td>\n",
       "      <td>realization</td>\n",
       "      <td>6.444050</td>\n",
       "      <td>7.4892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213415</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1929215</td>\n",
       "      <td>2010-09-09</td>\n",
       "      <td>Brian Orndorf</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>DVDTalk.com</td>\n",
       "      <td>Dreadfully formulaic and absent a thoughtful emotional core, the picture is best valued as a forgettable trifle starring Hollywood's most enduring weirdo.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Dreadfully formulaic and absent a thoughtful emotional core the picture is best valued as a forgettable trifle starring Hollywoods most enduring weirdo</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>24.614549</td>\n",
       "      <td>36.6750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213415</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1929215</td>\n",
       "      <td>2010-09-09</td>\n",
       "      <td>Brian Orndorf</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>DVDTalk.com</td>\n",
       "      <td>Dreadfully formulaic and absent a thoughtful emotional core, the picture is best valued as a forgettable trifle starring Hollywood's most enduring weirdo.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Dreadfully formulaic and absent a thoughtful emotional core the picture is best valued as a forgettable trifle starring Hollywoods most enduring weirdo</td>\n",
       "      <td>disappointment</td>\n",
       "      <td>21.926653</td>\n",
       "      <td>32.6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213415</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1929215</td>\n",
       "      <td>2010-09-09</td>\n",
       "      <td>Brian Orndorf</td>\n",
       "      <td>False</td>\n",
       "      <td>rotten</td>\n",
       "      <td>DVDTalk.com</td>\n",
       "      <td>Dreadfully formulaic and absent a thoughtful emotional core, the picture is best valued as a forgettable trifle starring Hollywood's most enduring weirdo.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>Dreadfully formulaic and absent a thoughtful emotional core the picture is best valued as a forgettable trifle starring Hollywoods most enduring weirdo</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>20.574082</td>\n",
       "      <td>30.6548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213416</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1924503</td>\n",
       "      <td>2010-08-19</td>\n",
       "      <td>Jules Brenner</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Cinema Signals</td>\n",
       "      <td>The success of the piece rests on Nivola's calm adaptability as fatherly hi-jinx tries his patience and makes its mark on his heart.</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>The success of the piece rests on Nivolas calm adaptability as fatherly hijinx tries his patience and makes its mark on his heart</td>\n",
       "      <td>approval</td>\n",
       "      <td>37.784600</td>\n",
       "      <td>47.3178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213416</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1924503</td>\n",
       "      <td>2010-08-19</td>\n",
       "      <td>Jules Brenner</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Cinema Signals</td>\n",
       "      <td>The success of the piece rests on Nivola's calm adaptability as fatherly hi-jinx tries his patience and makes its mark on his heart.</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>The success of the piece rests on Nivolas calm adaptability as fatherly hijinx tries his patience and makes its mark on his heart</td>\n",
       "      <td>admiration</td>\n",
       "      <td>33.104835</td>\n",
       "      <td>41.4574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213416</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1924503</td>\n",
       "      <td>2010-08-19</td>\n",
       "      <td>Jules Brenner</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Cinema Signals</td>\n",
       "      <td>The success of the piece rests on Nivola's calm adaptability as fatherly hi-jinx tries his patience and makes its mark on his heart.</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>The success of the piece rests on Nivolas calm adaptability as fatherly hijinx tries his patience and makes its mark on his heart</td>\n",
       "      <td>optimism</td>\n",
       "      <td>8.963305</td>\n",
       "      <td>11.2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213417</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1780984</td>\n",
       "      <td>2008-11-16</td>\n",
       "      <td>David Nusair</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Reel Film Reviews</td>\n",
       "      <td>...very amusing and agreeable...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>very amusing and agreeable</td>\n",
       "      <td>admiration</td>\n",
       "      <td>40.276130</td>\n",
       "      <td>44.8466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213417</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1780984</td>\n",
       "      <td>2008-11-16</td>\n",
       "      <td>David Nusair</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Reel Film Reviews</td>\n",
       "      <td>...very amusing and agreeable...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>very amusing and agreeable</td>\n",
       "      <td>amusement</td>\n",
       "      <td>30.382244</td>\n",
       "      <td>33.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213417</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>1780984</td>\n",
       "      <td>2008-11-16</td>\n",
       "      <td>David Nusair</td>\n",
       "      <td>False</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Reel Film Reviews</td>\n",
       "      <td>...very amusing and agreeable...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>very amusing and agreeable</td>\n",
       "      <td>joy</td>\n",
       "      <td>19.150305</td>\n",
       "      <td>21.3234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id               title  reviewId creationDate     criticName  \\\n",
       "1213414  $5_a_day  Five Dollars a Day   2097498   2012-08-01     Kevin Carr   \n",
       "1213414  $5_a_day  Five Dollars a Day   2097498   2012-08-01     Kevin Carr   \n",
       "1213414  $5_a_day  Five Dollars a Day   2097498   2012-08-01     Kevin Carr   \n",
       "1213415  $5_a_day  Five Dollars a Day   1929215   2010-09-09  Brian Orndorf   \n",
       "1213415  $5_a_day  Five Dollars a Day   1929215   2010-09-09  Brian Orndorf   \n",
       "1213415  $5_a_day  Five Dollars a Day   1929215   2010-09-09  Brian Orndorf   \n",
       "1213416  $5_a_day  Five Dollars a Day   1924503   2010-08-19  Jules Brenner   \n",
       "1213416  $5_a_day  Five Dollars a Day   1924503   2010-08-19  Jules Brenner   \n",
       "1213416  $5_a_day  Five Dollars a Day   1924503   2010-08-19  Jules Brenner   \n",
       "1213417  $5_a_day  Five Dollars a Day   1780984   2008-11-16   David Nusair   \n",
       "1213417  $5_a_day  Five Dollars a Day   1780984   2008-11-16   David Nusair   \n",
       "1213417  $5_a_day  Five Dollars a Day   1780984   2008-11-16   David Nusair   \n",
       "\n",
       "         isTopCritic reviewState     publicatioName  \\\n",
       "1213414        False      rotten        7M Pictures   \n",
       "1213414        False      rotten        7M Pictures   \n",
       "1213414        False      rotten        7M Pictures   \n",
       "1213415        False      rotten        DVDTalk.com   \n",
       "1213415        False      rotten        DVDTalk.com   \n",
       "1213415        False      rotten        DVDTalk.com   \n",
       "1213416        False       fresh     Cinema Signals   \n",
       "1213416        False       fresh     Cinema Signals   \n",
       "1213416        False       fresh     Cinema Signals   \n",
       "1213417        False       fresh  Reel Film Reviews   \n",
       "1213417        False       fresh  Reel Film Reviews   \n",
       "1213417        False       fresh  Reel Film Reviews   \n",
       "\n",
       "                                                                                                                                                         reviewText  \\\n",
       "1213414                                     $5 a Day isn't perfect, but it does examine some of the issues that we have when connecting with our parents as adults.   \n",
       "1213414                                     $5 a Day isn't perfect, but it does examine some of the issues that we have when connecting with our parents as adults.   \n",
       "1213414                                     $5 a Day isn't perfect, but it does examine some of the issues that we have when connecting with our parents as adults.   \n",
       "1213415  Dreadfully formulaic and absent a thoughtful emotional core, the picture is best valued as a forgettable trifle starring Hollywood's most enduring weirdo.   \n",
       "1213415  Dreadfully formulaic and absent a thoughtful emotional core, the picture is best valued as a forgettable trifle starring Hollywood's most enduring weirdo.   \n",
       "1213415  Dreadfully formulaic and absent a thoughtful emotional core, the picture is best valued as a forgettable trifle starring Hollywood's most enduring weirdo.   \n",
       "1213416                        The success of the piece rests on Nivola's calm adaptability as fatherly hi-jinx tries his patience and makes its mark on his heart.   \n",
       "1213416                        The success of the piece rests on Nivola's calm adaptability as fatherly hi-jinx tries his patience and makes its mark on his heart.   \n",
       "1213416                        The success of the piece rests on Nivola's calm adaptability as fatherly hi-jinx tries his patience and makes its mark on his heart.   \n",
       "1213417                                                                                                                            ...very amusing and agreeable...   \n",
       "1213417                                                                                                                            ...very amusing and agreeable...   \n",
       "1213417                                                                                                                            ...very amusing and agreeable...   \n",
       "\n",
       "        scoreSentiment  \\\n",
       "1213414       NEGATIVE   \n",
       "1213414       NEGATIVE   \n",
       "1213414       NEGATIVE   \n",
       "1213415       NEGATIVE   \n",
       "1213415       NEGATIVE   \n",
       "1213415       NEGATIVE   \n",
       "1213416       POSITIVE   \n",
       "1213416       POSITIVE   \n",
       "1213416       POSITIVE   \n",
       "1213417       POSITIVE   \n",
       "1213417       POSITIVE   \n",
       "1213417       POSITIVE   \n",
       "\n",
       "                                                                                                                                               cleanedReviewText  \\\n",
       "1213414                                      5 a Day isnt perfect but it does examine some of the issues that we have when connecting with our parents as adults   \n",
       "1213414                                      5 a Day isnt perfect but it does examine some of the issues that we have when connecting with our parents as adults   \n",
       "1213414                                      5 a Day isnt perfect but it does examine some of the issues that we have when connecting with our parents as adults   \n",
       "1213415  Dreadfully formulaic and absent a thoughtful emotional core the picture is best valued as a forgettable trifle starring Hollywoods most enduring weirdo   \n",
       "1213415  Dreadfully formulaic and absent a thoughtful emotional core the picture is best valued as a forgettable trifle starring Hollywoods most enduring weirdo   \n",
       "1213415  Dreadfully formulaic and absent a thoughtful emotional core the picture is best valued as a forgettable trifle starring Hollywoods most enduring weirdo   \n",
       "1213416                        The success of the piece rests on Nivolas calm adaptability as fatherly hijinx tries his patience and makes its mark on his heart   \n",
       "1213416                        The success of the piece rests on Nivolas calm adaptability as fatherly hijinx tries his patience and makes its mark on his heart   \n",
       "1213416                        The success of the piece rests on Nivolas calm adaptability as fatherly hijinx tries his patience and makes its mark on his heart   \n",
       "1213417                                                                                                                               very amusing and agreeable   \n",
       "1213417                                                                                                                               very amusing and agreeable   \n",
       "1213417                                                                                                                               very amusing and agreeable   \n",
       "\n",
       "                emotion  probability  probability_normalized  \n",
       "1213414     disapproval    60.371897                 70.1637  \n",
       "1213414        approval    19.228371                 22.3471  \n",
       "1213414     realization     6.444050                  7.4892  \n",
       "1213415       annoyance    24.614549                 36.6750  \n",
       "1213415  disappointment    21.926653                 32.6701  \n",
       "1213415     disapproval    20.574082                 30.6548  \n",
       "1213416        approval    37.784600                 47.3178  \n",
       "1213416      admiration    33.104835                 41.4574  \n",
       "1213416        optimism     8.963305                 11.2248  \n",
       "1213417      admiration    40.276130                 44.8466  \n",
       "1213417       amusement    30.382244                 33.8300  \n",
       "1213417             joy    19.150305                 21.3234  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "reviews_exploded.loc[reviews_exploded['id'] == '$5_a_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35af29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 3 emotions with the highest average per movie\n",
    "top_3_emotions = reviews_grouped.groupby('id').apply(\n",
    "    lambda x: x.nlargest(3, 'probability_normalized')\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c99350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>50.40925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>admiration</td>\n",
       "      <td>43.15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>36.67500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>56.85230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>50.76375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202807</th>\n",
       "      <td>zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000</td>\n",
       "      <td>approval</td>\n",
       "      <td>58.79150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202808</th>\n",
       "      <td>zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000</td>\n",
       "      <td>fear</td>\n",
       "      <td>33.92220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202809</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>amusement</td>\n",
       "      <td>57.87560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202810</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>joy</td>\n",
       "      <td>56.15785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202811</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>approval</td>\n",
       "      <td>48.20310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202812 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  id  \\\n",
       "0                                                           $5_a_day   \n",
       "1                                                           $5_a_day   \n",
       "2                                                           $5_a_day   \n",
       "3                                                      009_re_cyborg   \n",
       "4                                                      009_re_cyborg   \n",
       "...                                                              ...   \n",
       "202807  zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000   \n",
       "202808  zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000   \n",
       "202809                         zz_top_that_little_ol_band_from_texas   \n",
       "202810                         zz_top_that_little_ol_band_from_texas   \n",
       "202811                         zz_top_that_little_ol_band_from_texas   \n",
       "\n",
       "            emotion  probability_normalized  \n",
       "0       disapproval                50.40925  \n",
       "1        admiration                43.15200  \n",
       "2         annoyance                36.67500  \n",
       "3       disapproval                56.85230  \n",
       "4         annoyance                50.76375  \n",
       "...             ...                     ...  \n",
       "202807     approval                58.79150  \n",
       "202808         fear                33.92220  \n",
       "202809    amusement                57.87560  \n",
       "202810          joy                56.15785  \n",
       "202811     approval                48.20310  \n",
       "\n",
       "[202812 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce3b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id      emotion  probability_normalized\n",
      "0       $5_a_day  disapproval               38.706005\n",
      "1       $5_a_day   admiration               33.133632\n",
      "2       $5_a_day    annoyance               28.160362\n",
      "3  009_re_cyborg  disapproval               36.023280\n",
      "4  009_re_cyborg    annoyance               32.165396\n"
     ]
    }
   ],
   "source": [
    "# Normalize probabilities to sum 100% for each movie\n",
    "top_3_emotions['probability_normalized'] = top_3_emotions.groupby('id')['probability_normalized'].transform(\n",
    "    lambda x: (x / x.sum()) * 100\n",
    ")\n",
    "\n",
    "# Exibir o resultado\n",
    "print(top_3_emotions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87aeb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>38.706005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>admiration</td>\n",
       "      <td>33.133632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>28.160362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>36.023280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>32.165396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202807</th>\n",
       "      <td>zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000</td>\n",
       "      <td>approval</td>\n",
       "      <td>38.438228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202808</th>\n",
       "      <td>zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000</td>\n",
       "      <td>fear</td>\n",
       "      <td>22.178533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202809</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>amusement</td>\n",
       "      <td>35.673589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202810</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>joy</td>\n",
       "      <td>34.614795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202811</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>approval</td>\n",
       "      <td>29.711616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202812 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  id  \\\n",
       "0                                                           $5_a_day   \n",
       "1                                                           $5_a_day   \n",
       "2                                                           $5_a_day   \n",
       "3                                                      009_re_cyborg   \n",
       "4                                                      009_re_cyborg   \n",
       "...                                                              ...   \n",
       "202807  zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000   \n",
       "202808  zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000   \n",
       "202809                         zz_top_that_little_ol_band_from_texas   \n",
       "202810                         zz_top_that_little_ol_band_from_texas   \n",
       "202811                         zz_top_that_little_ol_band_from_texas   \n",
       "\n",
       "            emotion  probability_normalized  \n",
       "0       disapproval               38.706005  \n",
       "1        admiration               33.133632  \n",
       "2         annoyance               28.160362  \n",
       "3       disapproval               36.023280  \n",
       "4         annoyance               32.165396  \n",
       "...             ...                     ...  \n",
       "202807     approval               38.438228  \n",
       "202808         fear               22.178533  \n",
       "202809    amusement               35.673589  \n",
       "202810          joy               34.614795  \n",
       "202811     approval               29.711616  \n",
       "\n",
       "[202812 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c5373b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>[(disapproval, 38.70600543243529), (admiration, 33.1336321492672), (annoyance, 28.16036241829752)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>[(disapproval, 36.023279538211014), (annoyance, 32.165396240044096), (confusion, 31.81132422174489)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>[(annoyance, 41.840924718059824), (admiration, 36.49896509094363), (joy, 21.660110190996555)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[(admiration, 61.524699999999996), (approval, 35.5203), (love, 2.955)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-day</td>\n",
       "      <td>[(admiration, 41.199610661687295), (amusement, 32.26985631655974), (disappointment, 26.530533021752966)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67599</th>\n",
       "      <td>zus_and_zo_2003</td>\n",
       "      <td>[(desire, 40.01421221408237), (amusement, 30.742439443586793), (joy, 29.243348342330837)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67600</th>\n",
       "      <td>zvenigora</td>\n",
       "      <td>[(confusion, 71.48010000000001), (disapproval, 17.093200000000003), (annoyance, 11.426700000000002)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67601</th>\n",
       "      <td>zwei_mutter_2013</td>\n",
       "      <td>[(approval, 47.60089193324174), (admiration, 30.408156407361503), (realization, 21.990951659396757)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67602</th>\n",
       "      <td>zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000</td>\n",
       "      <td>[(sadness, 39.38323877121109), (approval, 38.43822776765831), (fear, 22.178533461130584)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67603</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>[(amusement, 35.673588966234796), (joy, 34.614795494603406), (approval, 29.7116155391618)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67604 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 id  \\\n",
       "0                                                          $5_a_day   \n",
       "1                                                     009_re_cyborg   \n",
       "2                                                            00_mhz   \n",
       "3                                                                 1   \n",
       "4                                                             1-day   \n",
       "...                                                             ...   \n",
       "67599                                               zus_and_zo_2003   \n",
       "67600                                                     zvenigora   \n",
       "67601                                              zwei_mutter_2013   \n",
       "67602  zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000   \n",
       "67603                         zz_top_that_little_ol_band_from_texas   \n",
       "\n",
       "                                                                                                       emotions  \n",
       "0            [(disapproval, 38.70600543243529), (admiration, 33.1336321492672), (annoyance, 28.16036241829752)]  \n",
       "1          [(disapproval, 36.023279538211014), (annoyance, 32.165396240044096), (confusion, 31.81132422174489)]  \n",
       "2                 [(annoyance, 41.840924718059824), (admiration, 36.49896509094363), (joy, 21.660110190996555)]  \n",
       "3                                        [(admiration, 61.524699999999996), (approval, 35.5203), (love, 2.955)]  \n",
       "4      [(admiration, 41.199610661687295), (amusement, 32.26985631655974), (disappointment, 26.530533021752966)]  \n",
       "...                                                                                                         ...  \n",
       "67599                 [(desire, 40.01421221408237), (amusement, 30.742439443586793), (joy, 29.243348342330837)]  \n",
       "67600      [(confusion, 71.48010000000001), (disapproval, 17.093200000000003), (annoyance, 11.426700000000002)]  \n",
       "67601      [(approval, 47.60089193324174), (admiration, 30.408156407361503), (realization, 21.990951659396757)]  \n",
       "67602                 [(sadness, 39.38323877121109), (approval, 38.43822776765831), (fear, 22.178533461130584)]  \n",
       "67603                [(amusement, 35.673588966234796), (joy, 34.614795494603406), (approval, 29.7116155391618)]  \n",
       "\n",
       "[67604 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'id' and create a list of tuples (emotion, probability_normalized)\n",
    "new_df = top_3_emotions.groupby('id').apply(\n",
    "    lambda x: list(zip(x['emotion'], x['probability_normalized'])))\n",
    "new_df = new_df.reset_index()\n",
    "new_df.columns = ['id', 'emotions']\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f727cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genre</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love_lies</td>\n",
       "      <td>Love, Lies</td>\n",
       "      <td>43.00</td>\n",
       "      <td>65.76</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Park Heung-Sik,Heung-Sik Park</td>\n",
       "      <td>Ha Young-Joon,Jeon Yun-su,Song Hye-jin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[(admiration, 78.20800919346894), (joy, 10.896268781721222), (approval, 10.89572202480984)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adrift_2018</td>\n",
       "      <td>Adrift</td>\n",
       "      <td>65.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Baltasar Kormákur</td>\n",
       "      <td>Aaron Kandell,Jordan Kandell,David Branson Smith</td>\n",
       "      <td>2018</td>\n",
       "      <td>[Adventure, Drama, Romance]</td>\n",
       "      <td>[(admiration, 42.079146438693414), (disappointment, 29.810255887131987), (approval, 28.1105976741746)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035316-born_to_kill</td>\n",
       "      <td>Born to Kill</td>\n",
       "      <td>74.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>92.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Robert Wise</td>\n",
       "      <td>Eve Greene,Richard Macaulay</td>\n",
       "      <td>1947</td>\n",
       "      <td>[Crime, Drama]</td>\n",
       "      <td>[(admiration, 41.51550238655111), (disgust, 38.18461517417403), (disappointment, 20.299882439274857)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garden_murder_case</td>\n",
       "      <td>The Garden Murder Case</td>\n",
       "      <td>55.67</td>\n",
       "      <td>65.76</td>\n",
       "      <td>61.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Edwin L. Marin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2016</td>\n",
       "      <td>[Thriller]</td>\n",
       "      <td>[(amusement, 47.11795941712488), (admiration, 30.561675917621006), (approval, 22.320364665254107)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>margarita_happy_hour</td>\n",
       "      <td>Margarita Happy Hour</td>\n",
       "      <td>55.67</td>\n",
       "      <td>76.00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Ilya Chaiken</td>\n",
       "      <td>Ilya Chaiken</td>\n",
       "      <td>2002</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[(desire, 45.54859553528897), (admiration, 29.028624793560592), (joy, 25.42277967115044)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67599</th>\n",
       "      <td>operation_goldenshell</td>\n",
       "      <td>Operation Goldenshell (Operación Concha)</td>\n",
       "      <td>55.67</td>\n",
       "      <td>65.76</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Antonio Cuadri</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[(amusement, 66.48219999999999), (realization, 20.131499999999996), (approval, 13.386299999999999)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67600</th>\n",
       "      <td>stag_night_of_the_dead</td>\n",
       "      <td>Stag Night of the Dead</td>\n",
       "      <td>43.00</td>\n",
       "      <td>65.76</td>\n",
       "      <td>81.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Neil Jones</td>\n",
       "      <td>Neil Jones</td>\n",
       "      <td>2016</td>\n",
       "      <td>[Horror, Action, Comedy]</td>\n",
       "      <td>[(approval, 60.283699999999996), (realization, 27.470399999999994), (admiration, 12.245899999999999)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67601</th>\n",
       "      <td>fun_size</td>\n",
       "      <td>Fun Size</td>\n",
       "      <td>47.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>86.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Josh Schwartz</td>\n",
       "      <td>Max Werner</td>\n",
       "      <td>2012</td>\n",
       "      <td>[Holiday, Comedy]</td>\n",
       "      <td>[(confusion, 48.71943200404655), (disappointment, 26.36692449980985), (amusement, 24.913643496143578)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67602</th>\n",
       "      <td>dassehra</td>\n",
       "      <td>Dassehra</td>\n",
       "      <td>55.67</td>\n",
       "      <td>65.76</td>\n",
       "      <td>131.0</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>Manish Vatsalya</td>\n",
       "      <td>Saurabh Choudhary</td>\n",
       "      <td>2019</td>\n",
       "      <td>[Action, Thriller]</td>\n",
       "      <td>[(approval, 71.40657548299487), (admiration, 20.783216447748366), (annoyance, 7.810208069256784)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67603</th>\n",
       "      <td>the-human-body</td>\n",
       "      <td>The Human Body</td>\n",
       "      <td>71.00</td>\n",
       "      <td>89.00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Peter Georgi</td>\n",
       "      <td>Richard Dale</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Documentary]</td>\n",
       "      <td>[(admiration, 36.33886394698251), (curiosity, 33.04143545613497), (optimism, 30.619700596882527)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67604 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id                                     title  \\\n",
       "0                   love_lies                                Love, Lies   \n",
       "1                 adrift_2018                                    Adrift   \n",
       "2        1035316-born_to_kill                              Born to Kill   \n",
       "3          garden_murder_case                    The Garden Murder Case   \n",
       "4        margarita_happy_hour                      Margarita Happy Hour   \n",
       "...                       ...                                       ...   \n",
       "67599   operation_goldenshell  Operation Goldenshell (Operación Concha)   \n",
       "67600  stag_night_of_the_dead                    Stag Night of the Dead   \n",
       "67601                fun_size                                  Fun Size   \n",
       "67602                dassehra                                  Dassehra   \n",
       "67603          the-human-body                            The Human Body   \n",
       "\n",
       "       audienceScore  tomatoMeter  runtimeMinutes originalLanguage  \\\n",
       "0              43.00        65.76           120.0           Korean   \n",
       "1              65.00        69.00           120.0          English   \n",
       "2              74.00        83.00            92.0          English   \n",
       "3              55.67        65.76            61.0          English   \n",
       "4              55.67        76.00            98.0          English   \n",
       "...              ...          ...             ...              ...   \n",
       "67599          55.67        65.76            88.0          Unknown   \n",
       "67600          43.00        65.76            81.0          English   \n",
       "67601          47.00        25.00            86.0          English   \n",
       "67602          55.67        65.76           131.0            Hindi   \n",
       "67603          71.00        89.00            43.0          English   \n",
       "\n",
       "                            director  \\\n",
       "0      Park Heung-Sik,Heung-Sik Park   \n",
       "1                  Baltasar Kormákur   \n",
       "2                        Robert Wise   \n",
       "3                     Edwin L. Marin   \n",
       "4                       Ilya Chaiken   \n",
       "...                              ...   \n",
       "67599                 Antonio Cuadri   \n",
       "67600                     Neil Jones   \n",
       "67601                  Josh Schwartz   \n",
       "67602                Manish Vatsalya   \n",
       "67603                   Peter Georgi   \n",
       "\n",
       "                                                 writer release_year  \\\n",
       "0                Ha Young-Joon,Jeon Yun-su,Song Hye-jin      Unknown   \n",
       "1      Aaron Kandell,Jordan Kandell,David Branson Smith         2018   \n",
       "2                           Eve Greene,Richard Macaulay         1947   \n",
       "3                                               Unknown         2016   \n",
       "4                                          Ilya Chaiken         2002   \n",
       "...                                                 ...          ...   \n",
       "67599                                           Unknown      Unknown   \n",
       "67600                                        Neil Jones         2016   \n",
       "67601                                        Max Werner         2012   \n",
       "67602                                 Saurabh Choudhary         2019   \n",
       "67603                                      Richard Dale      Unknown   \n",
       "\n",
       "                             genre  \\\n",
       "0                          [Drama]   \n",
       "1      [Adventure, Drama, Romance]   \n",
       "2                   [Crime, Drama]   \n",
       "3                       [Thriller]   \n",
       "4                          [Drama]   \n",
       "...                            ...   \n",
       "67599                    [Unknown]   \n",
       "67600     [Horror, Action, Comedy]   \n",
       "67601            [Holiday, Comedy]   \n",
       "67602           [Action, Thriller]   \n",
       "67603                [Documentary]   \n",
       "\n",
       "                                                                                                     emotions  \n",
       "0                 [(admiration, 78.20800919346894), (joy, 10.896268781721222), (approval, 10.89572202480984)]  \n",
       "1      [(admiration, 42.079146438693414), (disappointment, 29.810255887131987), (approval, 28.1105976741746)]  \n",
       "2       [(admiration, 41.51550238655111), (disgust, 38.18461517417403), (disappointment, 20.299882439274857)]  \n",
       "3          [(amusement, 47.11795941712488), (admiration, 30.561675917621006), (approval, 22.320364665254107)]  \n",
       "4                   [(desire, 45.54859553528897), (admiration, 29.028624793560592), (joy, 25.42277967115044)]  \n",
       "...                                                                                                       ...  \n",
       "67599     [(amusement, 66.48219999999999), (realization, 20.131499999999996), (approval, 13.386299999999999)]  \n",
       "67600   [(approval, 60.283699999999996), (realization, 27.470399999999994), (admiration, 12.245899999999999)]  \n",
       "67601  [(confusion, 48.71943200404655), (disappointment, 26.36692449980985), (amusement, 24.913643496143578)]  \n",
       "67602       [(approval, 71.40657548299487), (admiration, 20.783216447748366), (annoyance, 7.810208069256784)]  \n",
       "67603       [(admiration, 36.33886394698251), (curiosity, 33.04143545613497), (optimism, 30.619700596882527)]  \n",
       "\n",
       "[67604 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge movies and new_df\n",
    "\n",
    "merged_df = movies.merge(new_df, on='id', how='left')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43f488ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "title               0\n",
       "audienceScore       0\n",
       "tomatoMeter         0\n",
       "runtimeMinutes      0\n",
       "originalLanguage    0\n",
       "director            0\n",
       "writer              0\n",
       "release_year        0\n",
       "genre               0\n",
       "emotions            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38f8eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"movies_emotions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e64a5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/movies_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1851cabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genre</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love_lies</td>\n",
       "      <td>Love, Lies</td>\n",
       "      <td>43.00</td>\n",
       "      <td>65.76</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Park Heung-Sik,Heung-Sik Park</td>\n",
       "      <td>Ha Young-Joon,Jeon Yun-su,Song Hye-jin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>[('admiration', 78.20800919346894), ('joy', 10.896268781721222), ('approval', 10.89572202480984)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adrift_2018</td>\n",
       "      <td>Adrift</td>\n",
       "      <td>65.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Baltasar Kormákur</td>\n",
       "      <td>Aaron Kandell,Jordan Kandell,David Branson Smith</td>\n",
       "      <td>2018</td>\n",
       "      <td>['Adventure', 'Drama', 'Romance']</td>\n",
       "      <td>[('admiration', 42.079146438693414), ('disappointment', 29.810255887131987), ('approval', 28.1105976741746)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035316-born_to_kill</td>\n",
       "      <td>Born to Kill</td>\n",
       "      <td>74.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>92.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Robert Wise</td>\n",
       "      <td>Eve Greene,Richard Macaulay</td>\n",
       "      <td>1947</td>\n",
       "      <td>['Crime', 'Drama']</td>\n",
       "      <td>[('admiration', 41.51550238655111), ('disgust', 38.18461517417403), ('disappointment', 20.299882439274857)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garden_murder_case</td>\n",
       "      <td>The Garden Murder Case</td>\n",
       "      <td>55.67</td>\n",
       "      <td>65.76</td>\n",
       "      <td>61.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Edwin L. Marin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2016</td>\n",
       "      <td>['Thriller']</td>\n",
       "      <td>[('amusement', 47.11795941712488), ('admiration', 30.561675917621006), ('approval', 22.320364665254107)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>margarita_happy_hour</td>\n",
       "      <td>Margarita Happy Hour</td>\n",
       "      <td>55.67</td>\n",
       "      <td>76.00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Ilya Chaiken</td>\n",
       "      <td>Ilya Chaiken</td>\n",
       "      <td>2002</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>[('desire', 45.54859553528897), ('admiration', 29.028624793560592), ('joy', 25.42277967115044)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                   title  audienceScore  tomatoMeter  \\\n",
       "0             love_lies              Love, Lies          43.00        65.76   \n",
       "1           adrift_2018                  Adrift          65.00        69.00   \n",
       "2  1035316-born_to_kill            Born to Kill          74.00        83.00   \n",
       "3    garden_murder_case  The Garden Murder Case          55.67        65.76   \n",
       "4  margarita_happy_hour    Margarita Happy Hour          55.67        76.00   \n",
       "\n",
       "   runtimeMinutes originalLanguage                       director  \\\n",
       "0           120.0           Korean  Park Heung-Sik,Heung-Sik Park   \n",
       "1           120.0          English              Baltasar Kormákur   \n",
       "2            92.0          English                    Robert Wise   \n",
       "3            61.0          English                 Edwin L. Marin   \n",
       "4            98.0          English                   Ilya Chaiken   \n",
       "\n",
       "                                             writer release_year  \\\n",
       "0            Ha Young-Joon,Jeon Yun-su,Song Hye-jin      Unknown   \n",
       "1  Aaron Kandell,Jordan Kandell,David Branson Smith         2018   \n",
       "2                       Eve Greene,Richard Macaulay         1947   \n",
       "3                                           Unknown         2016   \n",
       "4                                      Ilya Chaiken         2002   \n",
       "\n",
       "                               genre  \\\n",
       "0                          ['Drama']   \n",
       "1  ['Adventure', 'Drama', 'Romance']   \n",
       "2                 ['Crime', 'Drama']   \n",
       "3                       ['Thriller']   \n",
       "4                          ['Drama']   \n",
       "\n",
       "                                                                                                       emotions  \n",
       "0             [('admiration', 78.20800919346894), ('joy', 10.896268781721222), ('approval', 10.89572202480984)]  \n",
       "1  [('admiration', 42.079146438693414), ('disappointment', 29.810255887131987), ('approval', 28.1105976741746)]  \n",
       "2   [('admiration', 41.51550238655111), ('disgust', 38.18461517417403), ('disappointment', 20.299882439274857)]  \n",
       "3      [('amusement', 47.11795941712488), ('admiration', 30.561675917621006), ('approval', 22.320364665254107)]  \n",
       "4               [('desire', 45.54859553528897), ('admiration', 29.028624793560592), ('joy', 25.42277967115044)]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668af82f",
   "metadata": {},
   "source": [
    "#### Normalizing movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af506d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize names\n",
    "def normalize_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s]', '', name)  # Remove special characters\n",
    "    name = name.lower().strip()  # Convert to lowercase and remove extra spaces\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0202e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating normalized title colunm in the movies df\n",
    "\n",
    "movies['title_normalized'] = movies['title'].apply(normalize_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506103b9",
   "metadata": {},
   "source": [
    "Delete duplicated normalized titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60300102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate titles (considering one entry per movie):\n",
      "                       id title_normalized\n",
      "15730              1_2013                1\n",
      "2681   1-one-human-minute                1\n",
      "66029              1_2017                1\n",
      "964                     1                1\n",
      "50302                 ten               10\n",
      "...                   ...              ...\n",
      "58403            zoo_2018              zoo\n",
      "15094           zoom_2016             zoom\n",
      "53027           zoom_2006             zoom\n",
      "40729                zulu             zulu\n",
      "52700           zulu_2013             zulu\n",
      "\n",
      "[9566 rows x 2 columns]\n",
      "All duplicate titles have been removed.\n",
      "\n",
      "Final DataFrame without duplicates:\n",
      "                                                                 id  \\\n",
      "49201                                          my_missing_valentine   \n",
      "53378                                                        00_mhz   \n",
      "16516                                                 009_re_cyborg   \n",
      "22292                                                            45   \n",
      "66029                                                        1_2017   \n",
      "...                                                             ...   \n",
      "26872                                                     zulu_dawn   \n",
      "63771                                                     zvenigora   \n",
      "43834  zycie_jako_smiertelna_choroba_przenoszona_droga_plciowa_2000   \n",
      "19024                         zz_top_that_little_ol_band_from_texas   \n",
      "43696                                                   burned_2006   \n",
      "\n",
      "                                                         title  audienceScore  \\\n",
      "49201                                                   消失的情人節          55.67   \n",
      "53378                                                  0.0 MHz          33.00   \n",
      "16516                                           009 Re: Cyborg          43.00   \n",
      "22292                                                     0.45          38.00   \n",
      "66029                                                       1%          82.00   \n",
      "...                                                        ...            ...   \n",
      "26872                                                Zulu Dawn          61.00   \n",
      "63771                                                Zvenigora          60.00   \n",
      "43834  Zycie Jako Smiertelna Choroba Przenoszona Droga Plciowa          62.00   \n",
      "19024                  ZZ Top: That Little Ol' Band From Texas          92.00   \n",
      "43696                                                    Zzyzx          40.00   \n",
      "\n",
      "       tomatoMeter  runtimeMinutes originalLanguage             director  \\\n",
      "49201        65.76           119.0          Chinese         Yu-Hsun Chen   \n",
      "53378        65.76           101.0           Korean         Yoo Sun-Dong   \n",
      "16516        23.00           103.0         Japanese       Kenji Kamiyama   \n",
      "22292        65.76           101.0          English          Gary Lennon   \n",
      "66029        47.00            88.0          Unknown     Stephen McCallum   \n",
      "...            ...             ...              ...                  ...   \n",
      "26872        50.00           121.0          English       Douglas Hickox   \n",
      "63771        65.76            90.0          Unknown  Aleksandr Dovzhenko   \n",
      "43834        65.76            88.0          Unknown    Krzysztof Zanussi   \n",
      "19024       100.00            90.0          English             Sam Dunn   \n",
      "43696        65.76            81.0          English      Richard Halpern   \n",
      "\n",
      "                                             writer release_year  \\\n",
      "49201                     Yu-Hsun Chen,Yu-Hsun Chen      Unknown   \n",
      "53378                                      Jang Jak         2020   \n",
      "16516                                Kenji Kamiyama         2015   \n",
      "22292                                   Gary Lennon         2020   \n",
      "66029                                       Unknown      Unknown   \n",
      "...                                             ...          ...   \n",
      "26872                                       Unknown         2005   \n",
      "63771  Aleksandr Dovzhenko,Mikhail Johansson,Yurtik      Unknown   \n",
      "43834                                       Unknown      Unknown   \n",
      "19024                        Sam Dunn,Ralph Chapman         2020   \n",
      "43696                              Art D'Alessandro         2007   \n",
      "\n",
      "                                                      genre  \\\n",
      "49201  ['Comedy', 'Drama', 'Fantasy', 'Foreign', 'Romance']   \n",
      "53378                                            ['Horror']   \n",
      "16516                     ['Action', 'Sci-fi', 'Animation']   \n",
      "22292                        ['Crime', 'Drama', 'Thriller']   \n",
      "66029                                           ['Unknown']   \n",
      "...                                                     ...   \n",
      "26872                                  ['History', 'Drama']   \n",
      "63771                                             ['Drama']   \n",
      "43834                                           ['Unknown']   \n",
      "19024                              ['Documentary', 'Music']   \n",
      "43696                                          ['Thriller']   \n",
      "\n",
      "                                                                                                           emotions  \\\n",
      "49201           [('admiration', 52.16256606223262), ('excitement', 24.98463494276986), ('joy', 22.852798994997528)]   \n",
      "53378           [('annoyance', 41.840924718059824), ('admiration', 36.49896509094363), ('joy', 21.660110190996555)]   \n",
      "16516    [('disapproval', 36.023279538211014), ('annoyance', 32.165396240044096), ('confusion', 31.81132422174489)]   \n",
      "22292                                            [('annoyance', 80.9424), ('anger', 14.1899), ('approval', 4.8677)]   \n",
      "66029  [('admiration', 40.5276835242493), ('disappointment', 30.096030871256218), ('approval', 29.376285604494484)]   \n",
      "...                                                                                                             ...   \n",
      "26872   [('approval', 42.721857728125535), ('admiration', 31.307022298011855), ('realization', 25.971119973862606)]   \n",
      "63771    [('confusion', 71.48010000000001), ('disapproval', 17.093200000000003), ('annoyance', 11.426700000000002)]   \n",
      "43834               [('sadness', 39.38323877121109), ('approval', 38.43822776765831), ('fear', 22.178533461130584)]   \n",
      "19024              [('amusement', 35.673588966234796), ('joy', 34.614795494603406), ('approval', 29.7116155391618)]   \n",
      "43696    [('approval', 37.162002902474896), ('disapproval', 34.991302315162024), ('confusion', 27.846694782363084)]   \n",
      "\n",
      "                                              title_normalized  count  \n",
      "49201                                                               3  \n",
      "53378                                                   00 mhz      3  \n",
      "16516                                            009 re cyborg     13  \n",
      "22292                                                      045      1  \n",
      "66029                                                        1     17  \n",
      "...                                                        ...    ...  \n",
      "26872                                                zulu dawn      4  \n",
      "63771                                                zvenigora      1  \n",
      "43834  zycie jako smiertelna choroba przenoszona droga plciowa      2  \n",
      "19024                    zz top that little ol band from texas      7  \n",
      "43696                                                    zzyzx      3  \n",
      "\n",
      "[61999 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group by 'id' and create the 'count' column\n",
    "# Group by 'id' and count occurrences in the reviews table\n",
    "reviews_count = reviews['id'].value_counts().reset_index()\n",
    "reviews_count.columns = ['id', 'count']\n",
    "\n",
    "# Add the 'count' column to the movies DataFrame\n",
    "movies = movies.merge(reviews_count, on='id', how='left')\n",
    "\n",
    "# Fill NaN values with 0 (movies with no reviews)\n",
    "movies['count'] = movies['count'].fillna(0)\n",
    "\n",
    "# Step 2: Check for duplicate titles\n",
    "# Create a DataFrame with one row per movie (using the first occurrence of each 'id')\n",
    "movies_unique = movies.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# Check for duplicate normalized titles\n",
    "duplicates = movies_unique[movies_unique['title_normalized'].duplicated(keep=False)]\n",
    "print(\"Duplicate titles (considering one entry per movie):\")\n",
    "print(duplicates[['id', 'title_normalized']].sort_values(by='title_normalized'))\n",
    "\n",
    "# Step 3: Remove movies with fewer occurrences\n",
    "# Sort movies by normalized title and review count\n",
    "movies_sorted = movies.sort_values(by=['title_normalized', 'count'], ascending=[True, False])\n",
    "\n",
    "# Keep only the first occurrence of each normalized title (the one with the most reviews)\n",
    "movies_no_duplicates = movies_sorted.drop_duplicates(subset=['title_normalized'], keep='first')\n",
    "\n",
    "# Check if there are still duplicates\n",
    "if movies_no_duplicates['title_normalized'].duplicated().any():\n",
    "    print(\"There are still duplicate titles after removal.\")\n",
    "else:\n",
    "    print(\"All duplicate titles have been removed.\")\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"\\nFinal DataFrame without duplicates:\")\n",
    "print(movies_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6374f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af0fc3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genre</th>\n",
       "      <th>emotions</th>\n",
       "      <th>title_normalized</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49201</th>\n",
       "      <td>my_missing_valentine</td>\n",
       "      <td>消失的情人節</td>\n",
       "      <td>55.67</td>\n",
       "      <td>65.76</td>\n",
       "      <td>119.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Yu-Hsun Chen</td>\n",
       "      <td>Yu-Hsun Chen,Yu-Hsun Chen</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Comedy', 'Drama', 'Fantasy', 'Foreign', 'Romance']</td>\n",
       "      <td>[('admiration', 52.16256606223262), ('excitement', 24.98463494276986), ('joy', 22.852798994997528)]</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53378</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>0.0 MHz</td>\n",
       "      <td>33.00</td>\n",
       "      <td>65.76</td>\n",
       "      <td>101.0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Yoo Sun-Dong</td>\n",
       "      <td>Jang Jak</td>\n",
       "      <td>2020</td>\n",
       "      <td>['Horror']</td>\n",
       "      <td>[('annoyance', 41.840924718059824), ('admiration', 36.49896509094363), ('joy', 21.660110190996555)]</td>\n",
       "      <td>00 mhz</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>009 Re: Cyborg</td>\n",
       "      <td>43.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>103.0</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Kenji Kamiyama</td>\n",
       "      <td>Kenji Kamiyama</td>\n",
       "      <td>2015</td>\n",
       "      <td>['Action', 'Sci-fi', 'Animation']</td>\n",
       "      <td>[('disapproval', 36.023279538211014), ('annoyance', 32.165396240044096), ('confusion', 31.81132422174489)]</td>\n",
       "      <td>009 re cyborg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22292</th>\n",
       "      <td>45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>38.00</td>\n",
       "      <td>65.76</td>\n",
       "      <td>101.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Gary Lennon</td>\n",
       "      <td>Gary Lennon</td>\n",
       "      <td>2020</td>\n",
       "      <td>['Crime', 'Drama', 'Thriller']</td>\n",
       "      <td>[('annoyance', 80.9424), ('anger', 14.1899), ('approval', 4.8677)]</td>\n",
       "      <td>045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66029</th>\n",
       "      <td>1_2017</td>\n",
       "      <td>1%</td>\n",
       "      <td>82.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Stephen McCallum</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Unknown']</td>\n",
       "      <td>[('admiration', 40.5276835242493), ('disappointment', 30.096030871256218), ('approval', 29.376285604494484)]</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id           title  audienceScore  tomatoMeter  \\\n",
       "49201  my_missing_valentine          消失的情人節          55.67        65.76   \n",
       "53378                00_mhz         0.0 MHz          33.00        65.76   \n",
       "16516         009_re_cyborg  009 Re: Cyborg          43.00        23.00   \n",
       "22292                    45            0.45          38.00        65.76   \n",
       "66029                1_2017              1%          82.00        47.00   \n",
       "\n",
       "       runtimeMinutes originalLanguage          director  \\\n",
       "49201           119.0          Chinese      Yu-Hsun Chen   \n",
       "53378           101.0           Korean      Yoo Sun-Dong   \n",
       "16516           103.0         Japanese    Kenji Kamiyama   \n",
       "22292           101.0          English       Gary Lennon   \n",
       "66029            88.0          Unknown  Stephen McCallum   \n",
       "\n",
       "                          writer release_year  \\\n",
       "49201  Yu-Hsun Chen,Yu-Hsun Chen      Unknown   \n",
       "53378                   Jang Jak         2020   \n",
       "16516             Kenji Kamiyama         2015   \n",
       "22292                Gary Lennon         2020   \n",
       "66029                    Unknown      Unknown   \n",
       "\n",
       "                                                      genre  \\\n",
       "49201  ['Comedy', 'Drama', 'Fantasy', 'Foreign', 'Romance']   \n",
       "53378                                            ['Horror']   \n",
       "16516                     ['Action', 'Sci-fi', 'Animation']   \n",
       "22292                        ['Crime', 'Drama', 'Thriller']   \n",
       "66029                                           ['Unknown']   \n",
       "\n",
       "                                                                                                           emotions  \\\n",
       "49201           [('admiration', 52.16256606223262), ('excitement', 24.98463494276986), ('joy', 22.852798994997528)]   \n",
       "53378           [('annoyance', 41.840924718059824), ('admiration', 36.49896509094363), ('joy', 21.660110190996555)]   \n",
       "16516    [('disapproval', 36.023279538211014), ('annoyance', 32.165396240044096), ('confusion', 31.81132422174489)]   \n",
       "22292                                            [('annoyance', 80.9424), ('anger', 14.1899), ('approval', 4.8677)]   \n",
       "66029  [('admiration', 40.5276835242493), ('disappointment', 30.096030871256218), ('approval', 29.376285604494484)]   \n",
       "\n",
       "      title_normalized  count  \n",
       "49201                       3  \n",
       "53378           00 mhz      3  \n",
       "16516    009 re cyborg     13  \n",
       "22292              045      1  \n",
       "66029                1     17  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eab803a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61999, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c11d9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genre</th>\n",
       "      <th>emotions</th>\n",
       "      <th>title_normalized</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60400</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>Five Dollars a Day</td>\n",
       "      <td>49.0</td>\n",
       "      <td>65.76</td>\n",
       "      <td>98.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Nigel Cole</td>\n",
       "      <td>Neal H. Dobrofsky,Tippi Dobrofsky</td>\n",
       "      <td>2010</td>\n",
       "      <td>['Comedy']</td>\n",
       "      <td>[('disapproval', 38.70600543243529), ('admiration', 33.1336321492672), ('annoyance', 28.16036241829752)]</td>\n",
       "      <td>five dollars a day</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id               title  audienceScore  tomatoMeter  \\\n",
       "60400  $5_a_day  Five Dollars a Day           49.0        65.76   \n",
       "\n",
       "       runtimeMinutes originalLanguage    director  \\\n",
       "60400            98.0          English  Nigel Cole   \n",
       "\n",
       "                                  writer release_year       genre  \\\n",
       "60400  Neal H. Dobrofsky,Tippi Dobrofsky         2010  ['Comedy']   \n",
       "\n",
       "                                                                                                       emotions  \\\n",
       "60400  [('disapproval', 38.70600543243529), ('admiration', 33.1336321492672), ('annoyance', 28.16036241829752)]   \n",
       "\n",
       "         title_normalized  count  \n",
       "60400  five dollars a day      4  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[movies['id'] == '$5_a_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d1af713",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.to_csv(\"movies_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639fb03",
   "metadata": {},
   "source": [
    "#### Transforming df into wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f77344e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>emotion</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$5_a_day</th>\n",
       "      <td>33.134</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.160</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>009_re_cyborg</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.165</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00_mhz</th>\n",
       "      <td>36.499</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.841</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.66</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.525</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-day</th>\n",
       "      <td>41.200</td>\n",
       "      <td>32.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "emotion        admiration  amusement  anger  annoyance  approval  caring  \\\n",
       "id                                                                         \n",
       "$5_a_day           33.134       0.00    0.0     28.160      0.00     0.0   \n",
       "009_re_cyborg       0.000       0.00    0.0     32.165      0.00     0.0   \n",
       "00_mhz             36.499       0.00    0.0     41.841      0.00     0.0   \n",
       "1                  61.525       0.00    0.0      0.000     35.52     0.0   \n",
       "1-day              41.200      32.27    0.0      0.000      0.00     0.0   \n",
       "\n",
       "emotion        confusion  curiosity  desire  disappointment  ...    joy  \\\n",
       "id                                                           ...          \n",
       "$5_a_day           0.000        0.0     0.0           0.000  ...   0.00   \n",
       "009_re_cyborg     31.811        0.0     0.0           0.000  ...   0.00   \n",
       "00_mhz             0.000        0.0     0.0           0.000  ...  21.66   \n",
       "1                  0.000        0.0     0.0           0.000  ...   0.00   \n",
       "1-day              0.000        0.0     0.0          26.531  ...   0.00   \n",
       "\n",
       "emotion         love  nervousness  optimism  pride  realization  relief  \\\n",
       "id                                                                        \n",
       "$5_a_day       0.000          0.0       0.0    0.0          0.0     0.0   \n",
       "009_re_cyborg  0.000          0.0       0.0    0.0          0.0     0.0   \n",
       "00_mhz         0.000          0.0       0.0    0.0          0.0     0.0   \n",
       "1              2.955          0.0       0.0    0.0          0.0     0.0   \n",
       "1-day          0.000          0.0       0.0    0.0          0.0     0.0   \n",
       "\n",
       "emotion        remorse  sadness  surprise  \n",
       "id                                         \n",
       "$5_a_day           0.0      0.0       0.0  \n",
       "009_re_cyborg      0.0      0.0       0.0  \n",
       "00_mhz             0.0      0.0       0.0  \n",
       "1                  0.0      0.0       0.0  \n",
       "1-day              0.0      0.0       0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforma df into wide format\n",
    "reviews_wide = top_3_emotions.pivot(index='id', columns='emotion', values='probability_normalized')\n",
    "\n",
    "# Fill missing values with 0\n",
    "reviews_wide = reviews_wide.fillna(0)\n",
    "\n",
    "reviews_wide = reviews_wide.round(3)  \n",
    "\n",
    "reviews_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "610db8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67604, 27)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311860cc",
   "metadata": {},
   "source": [
    "#### Removing IDs that are not present in the movies DataFrame from the reviews_wide DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f90d8055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61999, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>emotion</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$5_a_day</th>\n",
       "      <td>33.134</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>009_re_cyborg</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00_mhz</th>\n",
       "      <td>36.499</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-day</th>\n",
       "      <td>41.200</td>\n",
       "      <td>32.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-violent-women</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "emotion           admiration  amusement  anger  annoyance  approval  caring  \\\n",
       "id                                                                            \n",
       "$5_a_day              33.134       0.00    0.0     28.160       0.0     0.0   \n",
       "009_re_cyborg          0.000       0.00    0.0     32.165       0.0     0.0   \n",
       "00_mhz                36.499       0.00    0.0     41.841       0.0     0.0   \n",
       "1-day                 41.200      32.27    0.0      0.000       0.0     0.0   \n",
       "10-violent-women       0.000       0.00    0.0      0.000       0.0     0.0   \n",
       "\n",
       "emotion           confusion  curiosity  desire  disappointment  ...    joy  \\\n",
       "id                                                              ...          \n",
       "$5_a_day              0.000        0.0     0.0           0.000  ...   0.00   \n",
       "009_re_cyborg        31.811        0.0     0.0           0.000  ...   0.00   \n",
       "00_mhz                0.000        0.0     0.0           0.000  ...  21.66   \n",
       "1-day                 0.000        0.0     0.0          26.531  ...   0.00   \n",
       "10-violent-women      0.000        0.0     0.0          60.545  ...   0.00   \n",
       "\n",
       "emotion           love  nervousness  optimism  pride  realization  relief  \\\n",
       "id                                                                          \n",
       "$5_a_day           0.0          0.0       0.0    0.0          0.0     0.0   \n",
       "009_re_cyborg      0.0          0.0       0.0    0.0          0.0     0.0   \n",
       "00_mhz             0.0          0.0       0.0    0.0          0.0     0.0   \n",
       "1-day              0.0          0.0       0.0    0.0          0.0     0.0   \n",
       "10-violent-women   0.0          0.0       0.0    0.0          0.0     0.0   \n",
       "\n",
       "emotion           remorse  sadness  surprise  \n",
       "id                                            \n",
       "$5_a_day              0.0      0.0       0.0  \n",
       "009_re_cyborg         0.0      0.0       0.0  \n",
       "00_mhz                0.0      0.0       0.0  \n",
       "1-day                 0.0      0.0       0.0  \n",
       "10-violent-women      0.0      0.0       0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_wide = reviews_wide.loc[reviews_wide.index.isin(movies['id'])]\n",
    "\n",
    "print(reviews_wide.shape)\n",
    "\n",
    "reviews_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a067f",
   "metadata": {},
   "source": [
    "# Similarity Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeca3d6",
   "metadata": {},
   "source": [
    "Using PCA (Principal Component Analysis) to reduce data dimensionality before calculating similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edae01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate similarity matrix based on emotion data\n",
    "def calculate_similarity_matrix(reviews_wide):\n",
    "    # Apply PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=0.97)  # Keep 97% of the variance\n",
    "    reviews_reduced = pca.fit_transform(reviews_wide.values)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    sim_matrix = cosine_similarity(reviews_reduced)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    sim_df = pd.DataFrame(sim_matrix, index=reviews_wide.index, columns=reviews_wide.index)\n",
    "    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b62d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = calculate_similarity_matrix(reviews_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_bytes = sys.getsizeof(sim_df)\n",
    "\n",
    "size_gb = size_bytes / 1e9\n",
    "\n",
    "print(f\"Size of the matrix: {size_gb:.6f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980fbe4",
   "metadata": {},
   "source": [
    "# Converting sim_df Into a Smaller df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f083b1",
   "metadata": {},
   "source": [
    "#### Testing with a sample df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be635909",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sim_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1407c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df.to_csv(\"sample_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.dtypes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5f13fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'genre'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_subset = movies[['id', 'genre']].copy()\n",
    "movies_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ecc9ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/5\n",
      "After melt:\n",
      "             id1            id2     score\n",
      "0       $5_a_day       $5_a_day  1.000000\n",
      "1  009_re_cyborg       $5_a_day  0.642029\n",
      "2       $5_a_day  009_re_cyborg  0.642029\n",
      "3  009_re_cyborg  009_re_cyborg  1.000000\n",
      "4       $5_a_day         00_mhz  0.548673\n",
      "After merging with genres:\n",
      "             id1            id2     score             id  \\\n",
      "0       $5_a_day       $5_a_day  1.000000       $5_a_day   \n",
      "1  009_re_cyborg       $5_a_day  0.642029  009_re_cyborg   \n",
      "2       $5_a_day  009_re_cyborg  0.642029       $5_a_day   \n",
      "3  009_re_cyborg  009_re_cyborg  1.000000  009_re_cyborg   \n",
      "4       $5_a_day         00_mhz  0.548673       $5_a_day   \n",
      "\n",
      "                         genre           id_2                      genre_2  \n",
      "0                     [Comedy]       $5_a_day                     [Comedy]  \n",
      "1  [Action, Sci-fi, Animation]       $5_a_day                     [Comedy]  \n",
      "2                     [Comedy]  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "3  [Action, Sci-fi, Animation]  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "4                     [Comedy]         00_mhz                     [Horror]  \n",
      "Chunk 0 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks_sample/chunk_0.parquet\n",
      "Processing chunk 2/5\n",
      "After melt:\n",
      "      id1            id2     score\n",
      "0  00_mhz       $5_a_day  0.548673\n",
      "1   1-day       $5_a_day  0.015329\n",
      "2  00_mhz  009_re_cyborg  0.289133\n",
      "3   1-day  009_re_cyborg -0.307660\n",
      "4  00_mhz         00_mhz  1.000000\n",
      "After merging with genres:\n",
      "      id1            id2     score      id     genre           id_2  \\\n",
      "0  00_mhz       $5_a_day  0.548673  00_mhz  [Horror]       $5_a_day   \n",
      "1   1-day       $5_a_day  0.015329   1-day   [Drama]       $5_a_day   \n",
      "2  00_mhz  009_re_cyborg  0.289133  00_mhz  [Horror]  009_re_cyborg   \n",
      "3   1-day  009_re_cyborg -0.307660   1-day   [Drama]  009_re_cyborg   \n",
      "4  00_mhz         00_mhz  1.000000  00_mhz  [Horror]         00_mhz   \n",
      "\n",
      "                       genre_2  \n",
      "0                     [Comedy]  \n",
      "1                     [Comedy]  \n",
      "2  [Action, Sci-fi, Animation]  \n",
      "3  [Action, Sci-fi, Animation]  \n",
      "4                     [Horror]  \n",
      "Chunk 1 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks_sample/chunk_1.parquet\n",
      "Processing chunk 3/5\n",
      "After melt:\n",
      "                    id1            id2     score\n",
      "0      10-violent-women       $5_a_day  0.082268\n",
      "1  1000013_12_angry_men       $5_a_day -0.032376\n",
      "2      10-violent-women  009_re_cyborg  0.199621\n",
      "3  1000013_12_angry_men  009_re_cyborg -0.203014\n",
      "4      10-violent-women         00_mhz -0.193498\n",
      "After merging with genres:\n",
      "                    id1            id2     score                    id  \\\n",
      "0      10-violent-women       $5_a_day  0.082268      10-violent-women   \n",
      "1  1000013_12_angry_men       $5_a_day -0.032376  1000013_12_angry_men   \n",
      "2      10-violent-women  009_re_cyborg  0.199621      10-violent-women   \n",
      "3  1000013_12_angry_men  009_re_cyborg -0.203014  1000013_12_angry_men   \n",
      "4      10-violent-women         00_mhz -0.193498      10-violent-women   \n",
      "\n",
      "      genre           id_2                      genre_2  \n",
      "0  [Action]       $5_a_day                     [Comedy]  \n",
      "1   [Drama]       $5_a_day                     [Comedy]  \n",
      "2  [Action]  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "3   [Drama]  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "4  [Action]         00_mhz                     [Horror]  \n",
      "Chunk 2 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks_sample/chunk_2.parquet\n",
      "Processing chunk 4/5\n",
      "After melt:\n",
      "                 id1            id2     score\n",
      "0       10000292-rat       $5_a_day -0.253434\n",
      "1  10000594-guardian       $5_a_day  0.482700\n",
      "2       10000292-rat  009_re_cyborg -0.042702\n",
      "3  10000594-guardian  009_re_cyborg  0.536749\n",
      "4       10000292-rat         00_mhz -0.084409\n",
      "After merging with genres:\n",
      "                 id1            id2     score                 id      genre  \\\n",
      "0       10000292-rat       $5_a_day -0.253434       10000292-rat  [Fantasy]   \n",
      "1  10000594-guardian       $5_a_day  0.482700  10000594-guardian  [Fantasy]   \n",
      "2       10000292-rat  009_re_cyborg -0.042702       10000292-rat  [Fantasy]   \n",
      "3  10000594-guardian  009_re_cyborg  0.536749  10000594-guardian  [Fantasy]   \n",
      "4       10000292-rat         00_mhz -0.084409       10000292-rat  [Fantasy]   \n",
      "\n",
      "            id_2                      genre_2  \n",
      "0       $5_a_day                     [Comedy]  \n",
      "1       $5_a_day                     [Comedy]  \n",
      "2  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "3  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "4         00_mhz                     [Horror]  \n",
      "Chunk 3 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks_sample/chunk_3.parquet\n",
      "Processing chunk 5/5\n",
      "After melt:\n",
      "                       id1            id2     score\n",
      "0  10000604-porgy_and_bess       $5_a_day  0.130828\n",
      "1     10000633-corrections       $5_a_day -0.129166\n",
      "2  10000604-porgy_and_bess  009_re_cyborg -0.238149\n",
      "3     10000633-corrections  009_re_cyborg  0.108952\n",
      "4  10000604-porgy_and_bess         00_mhz  0.182596\n",
      "After merging with genres:\n",
      "                       id1            id2     score                       id  \\\n",
      "0  10000604-porgy_and_bess       $5_a_day  0.130828  10000604-porgy_and_bess   \n",
      "1     10000633-corrections       $5_a_day -0.129166     10000633-corrections   \n",
      "2  10000604-porgy_and_bess  009_re_cyborg -0.238149  10000604-porgy_and_bess   \n",
      "3     10000633-corrections  009_re_cyborg  0.108952     10000633-corrections   \n",
      "4  10000604-porgy_and_bess         00_mhz  0.182596  10000604-porgy_and_bess   \n",
      "\n",
      "                       genre           id_2                      genre_2  \n",
      "0  [Drama, Musical, Romance]       $5_a_day                     [Comedy]  \n",
      "1                  [Unknown]       $5_a_day                     [Comedy]  \n",
      "2  [Drama, Musical, Romance]  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "3                  [Unknown]  009_re_cyborg  [Action, Sci-fi, Animation]  \n",
      "4  [Drama, Musical, Romance]         00_mhz                     [Horror]  \n",
      "Chunk 4 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks_sample/chunk_4.parquet\n",
      "Concatenating all processed chunks...\n",
      "Final DataFrame:\n",
      "        id1                               id2     score     genre  \\\n",
      "0  $5_a_day                     hotel_de_love  0.995643  [Comedy]   \n",
      "1  $5_a_day                 the_midnight_gang  0.993423  [Comedy]   \n",
      "2  $5_a_day          ce_quil_ne_faut_pas_dire  0.989711  [Comedy]   \n",
      "3  $5_a_day                        simple_men  0.986981  [Comedy]   \n",
      "4  $5_a_day  handsome_a_netflix_mystery_movie  0.986540  [Comedy]   \n",
      "\n",
      "                    genre_2  \n",
      "0         [Romance, Comedy]  \n",
      "1                 [Unknown]  \n",
      "2                 [Unknown]  \n",
      "3  [Comedy, Drama, Romance]  \n",
      "4                  [Comedy]  \n",
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "# Directory to save processed chunks\n",
    "output_dir = '/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks_sample'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Function to check if two movies share at least one genre\n",
    "def has_common_genre(genres1, genres2):\n",
    "    # If one of the genres is 'Unknown', return True (consider any genre)\n",
    "    if 'Unknown' in genres1 or 'Unknown' in genres2:\n",
    "        return True\n",
    "    # Otherwise, check if there are common genres\n",
    "    return not set(genres1).isdisjoint(genres2)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk, movies):\n",
    "    # Reset the index to make 'id' a column\n",
    "    chunk = chunk.reset_index()\n",
    "    \n",
    "    # Step 2: Melt the matrix to transform it into pairs of IDs and similarity scores\n",
    "    chunk_melt = chunk.melt(id_vars='id', var_name='id2', value_name='score')\n",
    "    chunk_melt.rename(columns={'id': 'id1'}, inplace=True)\n",
    "    \n",
    "    # Convert columns to lighter data types to save memory\n",
    "    chunk_melt['id1'] = chunk_melt['id1'].astype('category')\n",
    "    chunk_melt['id2'] = chunk_melt['id2'].astype('category')\n",
    "    chunk_melt['score'] = chunk_melt['score'].astype('float32')\n",
    "    \n",
    "    # Debug: Check the result of melt\n",
    "    print(\"After melt:\")\n",
    "    print(chunk_melt.head())\n",
    "    \n",
    "    # Step 3: Inner join with the genres table to filter movies with at least one common genre\n",
    "    chunk_melt = chunk_melt.merge(movies_subset, left_on='id1', right_on='id', suffixes=('', '_1'))\n",
    "    chunk_melt = chunk_melt.merge(movies_subset, left_on='id2', right_on='id', suffixes=('', '_2'))\n",
    "    \n",
    "    # Debug: Check the result of merging\n",
    "    print(\"After merging with genres:\")\n",
    "    print(chunk_melt.head())\n",
    "    \n",
    "    # Filter movies that share at least one genre (or have 'Unknown')\n",
    "    chunk_melt = chunk_melt[chunk_melt.apply(lambda row: has_common_genre(row['genre'], row['genre_2']), axis=1)]\n",
    "    \n",
    "    # Step 4: Group by id1 and keep the top 6 most similar movies\n",
    "    chunk_melt = chunk_melt.sort_values(by=['id1', 'score'], ascending=[True, False])\n",
    "    chunk_melt = chunk_melt.groupby('id1').head(6)\n",
    "    \n",
    "    # Exclude the movie itself (where id1 == id2)\n",
    "    chunk_melt = chunk_melt[chunk_melt['id1'] != chunk_melt['id2']]\n",
    "    \n",
    "    # Keep only the top 5 similar movies\n",
    "    chunk_melt = chunk_melt.groupby('id1').head(5)\n",
    "    \n",
    "    # Return relevant columns, including genres\n",
    "    return chunk_melt[['id1', 'id2', 'score', 'genre', 'genre_2']]\n",
    "\n",
    "# Step 1: Split the sample DataFrame into chunks (e.g., 2 rows per chunk)\n",
    "chunk_size = 2  # Small chunk size for testing\n",
    "num_chunks = int(np.ceil(sample_df.shape[0] / chunk_size))\n",
    "\n",
    "# List to store paths of processed files\n",
    "processed_files = []\n",
    "\n",
    "# Process each chunk\n",
    "for i in range(num_chunks):\n",
    "    # Check if the chunk has already been processed\n",
    "    file_path = os.path.join(output_dir, f'chunk_{i}.parquet')\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Chunk {i} already processed. Skipping...\")\n",
    "        processed_files.append(file_path)\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing chunk {i + 1}/{num_chunks}\")\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, sample_df.shape[0])\n",
    "    chunk = sample_df.iloc[start:end, :]  # Select the current chunk\n",
    "    \n",
    "    # Process the chunk\n",
    "    result_chunk = process_chunk(chunk, movies)\n",
    "    \n",
    "    # Save the processed chunk to a Parquet file\n",
    "    result_chunk.to_parquet(file_path)\n",
    "    processed_files.append(file_path)\n",
    "    print(f\"Chunk {i} saved to {file_path}\")\n",
    "\n",
    "# Step 5: Concatenate all processed chunks\n",
    "print(\"Concatenating all processed chunks...\")\n",
    "df_final_sample = pd.concat([pd.read_parquet(file, columns=['id1', 'id2', 'score', 'genre', 'genre_2']) for file in processed_files], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"Final DataFrame:\")\n",
    "print(df_final_sample.head())\n",
    "\n",
    "# Save the final DataFrame (optional)\n",
    "df_final_sample.to_parquet(os.path.join(output_dir, 'final_result.parquet'))\n",
    "print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1b0015ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b7f804c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>score</th>\n",
       "      <th>genre</th>\n",
       "      <th>genre_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>hotel_de_love</td>\n",
       "      <td>0.995643</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>the_midnight_gang</td>\n",
       "      <td>0.993423</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>ce_quil_ne_faut_pas_dire</td>\n",
       "      <td>0.989711</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>simple_men</td>\n",
       "      <td>0.986981</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>handsome_a_netflix_mystery_movie</td>\n",
       "      <td>0.986540</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>the_lion_of_judah</td>\n",
       "      <td>0.990261</td>\n",
       "      <td>[Action, Sci-fi, Animation]</td>\n",
       "      <td>[Family, Holiday, Drama, Animation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>gog</td>\n",
       "      <td>0.974672</td>\n",
       "      <td>[Action, Sci-fi, Animation]</td>\n",
       "      <td>[Sci-fi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>hinda_and_her_sisterrrz</td>\n",
       "      <td>0.952611</td>\n",
       "      <td>[Action, Sci-fi, Animation]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>william_s_burroughs_commissioner_of_sewers</td>\n",
       "      <td>0.934580</td>\n",
       "      <td>[Action, Sci-fi, Animation]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>009_re_cyborg</td>\n",
       "      <td>malicedoll</td>\n",
       "      <td>0.920395</td>\n",
       "      <td>[Action, Sci-fi, Animation]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>exeter</td>\n",
       "      <td>0.996350</td>\n",
       "      <td>[Horror]</td>\n",
       "      <td>[Horror, Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>monster_seafood_wars</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>[Horror]</td>\n",
       "      <td>[Comedy, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>crooked_business</td>\n",
       "      <td>0.978435</td>\n",
       "      <td>[Horror]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>penumbra_2012</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>[Horror]</td>\n",
       "      <td>[Horror, Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00_mhz</td>\n",
       "      <td>the_dark_tapes</td>\n",
       "      <td>0.975301</td>\n",
       "      <td>[Horror]</td>\n",
       "      <td>[Horror, Sci-fi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1-day</td>\n",
       "      <td>eternal_beauty</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Romance, Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1-day</td>\n",
       "      <td>faces_of_women</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1-day</td>\n",
       "      <td>fanney_khan</td>\n",
       "      <td>0.999444</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Comedy, Drama, Musical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1-day</td>\n",
       "      <td>treading_water_2015</td>\n",
       "      <td>0.999428</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1-day</td>\n",
       "      <td>lost_illusions_2021</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Drama, History]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10-violent-women</td>\n",
       "      <td>the_amazing_bulk_2010</td>\n",
       "      <td>0.995868</td>\n",
       "      <td>[Action]</td>\n",
       "      <td>[Action, Fantasy, Romance, Animation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10-violent-women</td>\n",
       "      <td>son_of_sardaar_2012</td>\n",
       "      <td>0.962059</td>\n",
       "      <td>[Action]</td>\n",
       "      <td>[Action, Comedy, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10-violent-women</td>\n",
       "      <td>the_gauntlet_2013</td>\n",
       "      <td>0.957023</td>\n",
       "      <td>[Action]</td>\n",
       "      <td>[Action, Adventure, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10-violent-women</td>\n",
       "      <td>water_and_man</td>\n",
       "      <td>0.955834</td>\n",
       "      <td>[Action]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10-violent-women</td>\n",
       "      <td>field_diary</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>[Action]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1000013_12_angry_men</td>\n",
       "      <td>last_legion</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Action, History, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1000013_12_angry_men</td>\n",
       "      <td>together_together</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1000013_12_angry_men</td>\n",
       "      <td>max_payne</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Action, Crime, Drama, Fantasy, Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1000013_12_angry_men</td>\n",
       "      <td>the_last_duel_2021</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[History, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1000013_12_angry_men</td>\n",
       "      <td>cold_mountain</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Drama, War]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10000292-rat</td>\n",
       "      <td>borning</td>\n",
       "      <td>0.993044</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10000292-rat</td>\n",
       "      <td>sex_madness_revealed</td>\n",
       "      <td>0.988937</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Comedy, Fantasy, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10000292-rat</td>\n",
       "      <td>shrek_the_halls</td>\n",
       "      <td>0.966861</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Family, Holiday, Adventure, Fantasy, Comedy, Animation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10000292-rat</td>\n",
       "      <td>one_magic_christmas</td>\n",
       "      <td>0.956940</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Family, Holiday, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10000292-rat</td>\n",
       "      <td>the_king_and_the_mockingbird</td>\n",
       "      <td>0.938886</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Family, Fantasy, Animation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10000594-guardian</td>\n",
       "      <td>the-land-of-college-prophets</td>\n",
       "      <td>0.957420</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Action, Fantasy, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10000594-guardian</td>\n",
       "      <td>legend_of_frosty_the_snowman</td>\n",
       "      <td>0.948520</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Family, Holiday, Fantasy, Comedy, Animation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10000594-guardian</td>\n",
       "      <td>batteries_not_included</td>\n",
       "      <td>0.942098</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Family, Sci-fi, Fantasy, Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10000594-guardian</td>\n",
       "      <td>until_we_meet_again_2022</td>\n",
       "      <td>0.934966</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Romance, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10000594-guardian</td>\n",
       "      <td>train_ride_to_hollywood</td>\n",
       "      <td>0.933614</td>\n",
       "      <td>[Fantasy]</td>\n",
       "      <td>[Comedy, Drama, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10000604-porgy_and_bess</td>\n",
       "      <td>actual_people</td>\n",
       "      <td>0.998632</td>\n",
       "      <td>[Drama, Musical, Romance]</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10000604-porgy_and_bess</td>\n",
       "      <td>sacred-stage---the-mariinsky-theater</td>\n",
       "      <td>0.997631</td>\n",
       "      <td>[Drama, Musical, Romance]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10000604-porgy_and_bess</td>\n",
       "      <td>intruder_in_the_dust_1949</td>\n",
       "      <td>0.996995</td>\n",
       "      <td>[Drama, Musical, Romance]</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10000604-porgy_and_bess</td>\n",
       "      <td>1101040-terrorist</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>[Drama, Musical, Romance]</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10000604-porgy_and_bess</td>\n",
       "      <td>the_carpet_from_bagdad</td>\n",
       "      <td>0.993974</td>\n",
       "      <td>[Drama, Musical, Romance]</td>\n",
       "      <td>[Adventure, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10000633-corrections</td>\n",
       "      <td>first_winter_2012</td>\n",
       "      <td>0.996480</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10000633-corrections</td>\n",
       "      <td>for_ever_mozart</td>\n",
       "      <td>0.993640</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10000633-corrections</td>\n",
       "      <td>road_kill_2010</td>\n",
       "      <td>0.993329</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Horror, Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10000633-corrections</td>\n",
       "      <td>building_the_american_dream</td>\n",
       "      <td>0.993021</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Documentary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10000633-corrections</td>\n",
       "      <td>rudolph_and_frosty_christmas_in_july</td>\n",
       "      <td>0.992635</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Family, Holiday, Fantasy, Animation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id1                                         id2  \\\n",
       "0                  $5_a_day                               hotel_de_love   \n",
       "1                  $5_a_day                           the_midnight_gang   \n",
       "2                  $5_a_day                    ce_quil_ne_faut_pas_dire   \n",
       "3                  $5_a_day                                  simple_men   \n",
       "4                  $5_a_day            handsome_a_netflix_mystery_movie   \n",
       "5             009_re_cyborg                           the_lion_of_judah   \n",
       "6             009_re_cyborg                                         gog   \n",
       "7             009_re_cyborg                     hinda_and_her_sisterrrz   \n",
       "8             009_re_cyborg  william_s_burroughs_commissioner_of_sewers   \n",
       "9             009_re_cyborg                                  malicedoll   \n",
       "10                   00_mhz                                      exeter   \n",
       "11                   00_mhz                        monster_seafood_wars   \n",
       "12                   00_mhz                            crooked_business   \n",
       "13                   00_mhz                               penumbra_2012   \n",
       "14                   00_mhz                              the_dark_tapes   \n",
       "15                    1-day                              eternal_beauty   \n",
       "16                    1-day                              faces_of_women   \n",
       "17                    1-day                                 fanney_khan   \n",
       "18                    1-day                         treading_water_2015   \n",
       "19                    1-day                         lost_illusions_2021   \n",
       "20         10-violent-women                       the_amazing_bulk_2010   \n",
       "21         10-violent-women                         son_of_sardaar_2012   \n",
       "22         10-violent-women                           the_gauntlet_2013   \n",
       "23         10-violent-women                               water_and_man   \n",
       "24         10-violent-women                                 field_diary   \n",
       "25     1000013_12_angry_men                                 last_legion   \n",
       "26     1000013_12_angry_men                           together_together   \n",
       "27     1000013_12_angry_men                                   max_payne   \n",
       "28     1000013_12_angry_men                          the_last_duel_2021   \n",
       "29     1000013_12_angry_men                               cold_mountain   \n",
       "30             10000292-rat                                     borning   \n",
       "31             10000292-rat                        sex_madness_revealed   \n",
       "32             10000292-rat                             shrek_the_halls   \n",
       "33             10000292-rat                         one_magic_christmas   \n",
       "34             10000292-rat                the_king_and_the_mockingbird   \n",
       "35        10000594-guardian                the-land-of-college-prophets   \n",
       "36        10000594-guardian                legend_of_frosty_the_snowman   \n",
       "37        10000594-guardian                      batteries_not_included   \n",
       "38        10000594-guardian                    until_we_meet_again_2022   \n",
       "39        10000594-guardian                     train_ride_to_hollywood   \n",
       "40  10000604-porgy_and_bess                               actual_people   \n",
       "41  10000604-porgy_and_bess        sacred-stage---the-mariinsky-theater   \n",
       "42  10000604-porgy_and_bess                   intruder_in_the_dust_1949   \n",
       "43  10000604-porgy_and_bess                           1101040-terrorist   \n",
       "44  10000604-porgy_and_bess                      the_carpet_from_bagdad   \n",
       "45     10000633-corrections                           first_winter_2012   \n",
       "46     10000633-corrections                             for_ever_mozart   \n",
       "47     10000633-corrections                              road_kill_2010   \n",
       "48     10000633-corrections                 building_the_american_dream   \n",
       "49     10000633-corrections        rudolph_and_frosty_christmas_in_july   \n",
       "\n",
       "       score                        genre  \\\n",
       "0   0.995643                     [Comedy]   \n",
       "1   0.993423                     [Comedy]   \n",
       "2   0.989711                     [Comedy]   \n",
       "3   0.986981                     [Comedy]   \n",
       "4   0.986540                     [Comedy]   \n",
       "5   0.990261  [Action, Sci-fi, Animation]   \n",
       "6   0.974672  [Action, Sci-fi, Animation]   \n",
       "7   0.952611  [Action, Sci-fi, Animation]   \n",
       "8   0.934580  [Action, Sci-fi, Animation]   \n",
       "9   0.920395  [Action, Sci-fi, Animation]   \n",
       "10  0.996350                     [Horror]   \n",
       "11  0.991340                     [Horror]   \n",
       "12  0.978435                     [Horror]   \n",
       "13  0.977618                     [Horror]   \n",
       "14  0.975301                     [Horror]   \n",
       "15  0.999993                      [Drama]   \n",
       "16  0.999945                      [Drama]   \n",
       "17  0.999444                      [Drama]   \n",
       "18  0.999428                      [Drama]   \n",
       "19  0.998936                      [Drama]   \n",
       "20  0.995868                     [Action]   \n",
       "21  0.962059                     [Action]   \n",
       "22  0.957023                     [Action]   \n",
       "23  0.955834                     [Action]   \n",
       "24  0.955717                     [Action]   \n",
       "25  0.999996                      [Drama]   \n",
       "26  0.999923                      [Drama]   \n",
       "27  0.999850                      [Drama]   \n",
       "28  0.999778                      [Drama]   \n",
       "29  0.999775                      [Drama]   \n",
       "30  0.993044                    [Fantasy]   \n",
       "31  0.988937                    [Fantasy]   \n",
       "32  0.966861                    [Fantasy]   \n",
       "33  0.956940                    [Fantasy]   \n",
       "34  0.938886                    [Fantasy]   \n",
       "35  0.957420                    [Fantasy]   \n",
       "36  0.948520                    [Fantasy]   \n",
       "37  0.942098                    [Fantasy]   \n",
       "38  0.934966                    [Fantasy]   \n",
       "39  0.933614                    [Fantasy]   \n",
       "40  0.998632    [Drama, Musical, Romance]   \n",
       "41  0.997631    [Drama, Musical, Romance]   \n",
       "42  0.996995    [Drama, Musical, Romance]   \n",
       "43  0.996298    [Drama, Musical, Romance]   \n",
       "44  0.993974    [Drama, Musical, Romance]   \n",
       "45  0.996480                    [Unknown]   \n",
       "46  0.993640                    [Unknown]   \n",
       "47  0.993329                    [Unknown]   \n",
       "48  0.993021                    [Unknown]   \n",
       "49  0.992635                    [Unknown]   \n",
       "\n",
       "                                                     genre_2  \n",
       "0                                          [Romance, Comedy]  \n",
       "1                                                  [Unknown]  \n",
       "2                                                  [Unknown]  \n",
       "3                                   [Comedy, Drama, Romance]  \n",
       "4                                                   [Comedy]  \n",
       "5                        [Family, Holiday, Drama, Animation]  \n",
       "6                                                   [Sci-fi]  \n",
       "7                                                  [Unknown]  \n",
       "8                                                  [Unknown]  \n",
       "9                                                  [Unknown]  \n",
       "10                                        [Horror, Thriller]  \n",
       "11                                          [Comedy, Horror]  \n",
       "12                                                 [Unknown]  \n",
       "13                                        [Horror, Thriller]  \n",
       "14                                          [Horror, Sci-fi]  \n",
       "15                                  [Romance, Comedy, Drama]  \n",
       "16                                           [Comedy, Drama]  \n",
       "17                                  [Comedy, Drama, Musical]  \n",
       "18                                           [Comedy, Drama]  \n",
       "19                                          [Drama, History]  \n",
       "20                     [Action, Fantasy, Romance, Animation]  \n",
       "21                                 [Action, Comedy, Romance]  \n",
       "22                               [Action, Adventure, Horror]  \n",
       "23                                                 [Unknown]  \n",
       "24                                                 [Unknown]  \n",
       "25                                  [Action, History, Drama]  \n",
       "26                                           [Comedy, Drama]  \n",
       "27                 [Action, Crime, Drama, Fantasy, Thriller]  \n",
       "28                                          [History, Drama]  \n",
       "29                                              [Drama, War]  \n",
       "30                                                 [Unknown]  \n",
       "31                                 [Comedy, Fantasy, Horror]  \n",
       "32  [Family, Holiday, Adventure, Fantasy, Comedy, Animation]  \n",
       "33                                [Family, Holiday, Fantasy]  \n",
       "34                              [Family, Fantasy, Animation]  \n",
       "35                                 [Action, Fantasy, Horror]  \n",
       "36             [Family, Holiday, Fantasy, Comedy, Animation]  \n",
       "37                         [Family, Sci-fi, Fantasy, Comedy]  \n",
       "38                                        [Romance, Fantasy]  \n",
       "39                                  [Comedy, Drama, Fantasy]  \n",
       "40                                                   [Drama]  \n",
       "41                                                 [Unknown]  \n",
       "42                                                   [Drama]  \n",
       "43                                                   [Drama]  \n",
       "44                                        [Adventure, Drama]  \n",
       "45                                                   [Drama]  \n",
       "46                                           [Comedy, Drama]  \n",
       "47                                        [Horror, Thriller]  \n",
       "48                                             [Documentary]  \n",
       "49                     [Family, Holiday, Fantasy, Animation]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbe77a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/yz06rsgd4g58pv0x59d7qyc40000gn/T/ipykernel_84867/993239469.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movies['genre'] = movies['genre'].apply(ast.literal_eval)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'genre' from str into list\n",
    "movies['genre'] = movies['genre'].apply(ast.literal_eval)\n",
    "movies_subset['genre'] = movies_subset['genre'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2882cbca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check the 'genre' type after conversion\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in movies after conversion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(movies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in movies_subset after conversion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(movies_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the 'genre' type after conversion\n",
    "print(\"Type of 'genre' in movies after conversion:\", type(movies['genre'].iloc[0]))\n",
    "print(\"Type of 'genre' in movies_subset after conversion:\", type(movies_subset['genre'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f8122",
   "metadata": {},
   "source": [
    "#### Applying it to the real sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a997631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 0 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_0.parquet\n",
      "Processing chunk 2/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 1 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_1.parquet\n",
      "Processing chunk 3/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 2 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_2.parquet\n",
      "Processing chunk 4/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 3 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_3.parquet\n",
      "Processing chunk 5/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 4 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_4.parquet\n",
      "Processing chunk 6/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 5 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_5.parquet\n",
      "Processing chunk 7/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 6 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_6.parquet\n",
      "Processing chunk 8/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 7 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_7.parquet\n",
      "Processing chunk 9/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 8 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_8.parquet\n",
      "Processing chunk 10/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 9 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_9.parquet\n",
      "Processing chunk 11/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 10 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_10.parquet\n",
      "Processing chunk 12/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 11 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_11.parquet\n",
      "Processing chunk 13/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 12 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_12.parquet\n",
      "Processing chunk 14/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 13 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_13.parquet\n",
      "Processing chunk 15/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 14 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_14.parquet\n",
      "Processing chunk 16/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 15 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_15.parquet\n",
      "Processing chunk 17/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 16 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_16.parquet\n",
      "Processing chunk 18/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 17 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_17.parquet\n",
      "Processing chunk 19/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 18 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_18.parquet\n",
      "Processing chunk 20/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 19 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_19.parquet\n",
      "Processing chunk 21/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 20 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_20.parquet\n",
      "Processing chunk 22/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 21 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_21.parquet\n",
      "Processing chunk 23/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 22 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_22.parquet\n",
      "Processing chunk 24/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 23 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_23.parquet\n",
      "Processing chunk 25/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 24 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_24.parquet\n",
      "Processing chunk 26/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 25 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_25.parquet\n",
      "Processing chunk 27/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 26 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_26.parquet\n",
      "Processing chunk 28/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 27 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_27.parquet\n",
      "Processing chunk 29/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 28 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_28.parquet\n",
      "Processing chunk 30/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 29 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_29.parquet\n",
      "Processing chunk 31/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 30 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_30.parquet\n",
      "Processing chunk 32/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 31 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_31.parquet\n",
      "Processing chunk 33/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 32 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_32.parquet\n",
      "Processing chunk 34/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 33 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_33.parquet\n",
      "Processing chunk 35/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 34 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_34.parquet\n",
      "Processing chunk 36/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 35 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_35.parquet\n",
      "Processing chunk 37/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 36 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_36.parquet\n",
      "Processing chunk 38/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 37 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_37.parquet\n",
      "Processing chunk 39/207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 38 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_38.parquet\n",
      "Processing chunk 40/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 39 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_39.parquet\n",
      "Processing chunk 41/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 40 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_40.parquet\n",
      "Processing chunk 42/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 41 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_41.parquet\n",
      "Processing chunk 43/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 42 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_42.parquet\n",
      "Processing chunk 44/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 43 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_43.parquet\n",
      "Processing chunk 45/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 44 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_44.parquet\n",
      "Processing chunk 46/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 45 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_45.parquet\n",
      "Processing chunk 47/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 46 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_46.parquet\n",
      "Processing chunk 48/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 47 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_47.parquet\n",
      "Processing chunk 49/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 48 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_48.parquet\n",
      "Processing chunk 50/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 49 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_49.parquet\n",
      "Processing chunk 51/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 50 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_50.parquet\n",
      "Processing chunk 52/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 51 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_51.parquet\n",
      "Processing chunk 53/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 52 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_52.parquet\n",
      "Processing chunk 54/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 53 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_53.parquet\n",
      "Processing chunk 55/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 54 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_54.parquet\n",
      "Processing chunk 56/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 55 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_55.parquet\n",
      "Processing chunk 57/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 56 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_56.parquet\n",
      "Processing chunk 58/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 57 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_57.parquet\n",
      "Processing chunk 59/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 58 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_58.parquet\n",
      "Processing chunk 60/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 59 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_59.parquet\n",
      "Processing chunk 61/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 60 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_60.parquet\n",
      "Processing chunk 62/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 61 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_61.parquet\n",
      "Processing chunk 63/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 62 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_62.parquet\n",
      "Processing chunk 64/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 63 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_63.parquet\n",
      "Processing chunk 65/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 64 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_64.parquet\n",
      "Processing chunk 66/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 65 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_65.parquet\n",
      "Processing chunk 67/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 66 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_66.parquet\n",
      "Processing chunk 68/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 67 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_67.parquet\n",
      "Processing chunk 69/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 68 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_68.parquet\n",
      "Processing chunk 70/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 69 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_69.parquet\n",
      "Processing chunk 71/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 70 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_70.parquet\n",
      "Processing chunk 72/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 71 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_71.parquet\n",
      "Processing chunk 73/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 72 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_72.parquet\n",
      "Processing chunk 74/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 73 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_73.parquet\n",
      "Processing chunk 75/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 74 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_74.parquet\n",
      "Processing chunk 76/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 75 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_75.parquet\n",
      "Processing chunk 77/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 76 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_76.parquet\n",
      "Processing chunk 78/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 77 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_77.parquet\n",
      "Processing chunk 79/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 78 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_78.parquet\n",
      "Processing chunk 80/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 79 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_79.parquet\n",
      "Processing chunk 81/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 80 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_80.parquet\n",
      "Processing chunk 82/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 81 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_81.parquet\n",
      "Processing chunk 83/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 82 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_82.parquet\n",
      "Processing chunk 84/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 83 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_83.parquet\n",
      "Processing chunk 85/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 84 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_84.parquet\n",
      "Processing chunk 86/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 85 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_85.parquet\n",
      "Processing chunk 87/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 86 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_86.parquet\n",
      "Processing chunk 88/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 87 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_87.parquet\n",
      "Processing chunk 89/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 88 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_88.parquet\n",
      "Processing chunk 90/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 89 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_89.parquet\n",
      "Processing chunk 91/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 90 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_90.parquet\n",
      "Processing chunk 92/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 91 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_91.parquet\n",
      "Processing chunk 93/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 92 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_92.parquet\n",
      "Processing chunk 94/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 93 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_93.parquet\n",
      "Processing chunk 95/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 94 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_94.parquet\n",
      "Processing chunk 96/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 95 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_95.parquet\n",
      "Processing chunk 97/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 96 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_96.parquet\n",
      "Processing chunk 98/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 97 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_97.parquet\n",
      "Processing chunk 99/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 98 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_98.parquet\n",
      "Processing chunk 100/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 99 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_99.parquet\n",
      "Processing chunk 101/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 100 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_100.parquet\n",
      "Processing chunk 102/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 101 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_101.parquet\n",
      "Processing chunk 103/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 102 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_102.parquet\n",
      "Processing chunk 104/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 103 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_103.parquet\n",
      "Processing chunk 105/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 104 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_104.parquet\n",
      "Processing chunk 106/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 105 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_105.parquet\n",
      "Processing chunk 107/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 106 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_106.parquet\n",
      "Processing chunk 108/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 107 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_107.parquet\n",
      "Processing chunk 109/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 108 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_108.parquet\n",
      "Processing chunk 110/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 109 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_109.parquet\n",
      "Processing chunk 111/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 110 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_110.parquet\n",
      "Processing chunk 112/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 111 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_111.parquet\n",
      "Processing chunk 113/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 112 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_112.parquet\n",
      "Processing chunk 114/207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 113 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_113.parquet\n",
      "Processing chunk 115/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 114 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_114.parquet\n",
      "Processing chunk 116/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 115 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_115.parquet\n",
      "Processing chunk 117/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 116 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_116.parquet\n",
      "Processing chunk 118/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 117 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_117.parquet\n",
      "Processing chunk 119/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 118 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_118.parquet\n",
      "Processing chunk 120/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 119 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_119.parquet\n",
      "Processing chunk 121/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 120 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_120.parquet\n",
      "Processing chunk 122/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 121 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_121.parquet\n",
      "Processing chunk 123/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 122 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_122.parquet\n",
      "Processing chunk 124/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 123 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_123.parquet\n",
      "Processing chunk 125/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 124 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_124.parquet\n",
      "Processing chunk 126/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 125 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_125.parquet\n",
      "Processing chunk 127/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 126 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_126.parquet\n",
      "Processing chunk 128/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 127 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_127.parquet\n",
      "Processing chunk 129/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 128 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_128.parquet\n",
      "Processing chunk 130/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 129 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_129.parquet\n",
      "Processing chunk 131/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 130 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_130.parquet\n",
      "Processing chunk 132/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 131 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_131.parquet\n",
      "Processing chunk 133/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 132 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_132.parquet\n",
      "Processing chunk 134/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 133 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_133.parquet\n",
      "Processing chunk 135/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 134 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_134.parquet\n",
      "Processing chunk 136/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 135 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_135.parquet\n",
      "Processing chunk 137/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 136 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_136.parquet\n",
      "Processing chunk 138/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 137 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_137.parquet\n",
      "Processing chunk 139/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 138 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_138.parquet\n",
      "Processing chunk 140/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 139 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_139.parquet\n",
      "Processing chunk 141/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 140 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_140.parquet\n",
      "Processing chunk 142/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 141 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_141.parquet\n",
      "Processing chunk 143/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 142 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_142.parquet\n",
      "Processing chunk 144/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 143 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_143.parquet\n",
      "Processing chunk 145/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 144 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_144.parquet\n",
      "Processing chunk 146/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 145 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_145.parquet\n",
      "Processing chunk 147/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 146 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_146.parquet\n",
      "Processing chunk 148/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 147 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_147.parquet\n",
      "Processing chunk 149/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 148 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_148.parquet\n",
      "Processing chunk 150/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 149 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_149.parquet\n",
      "Processing chunk 151/207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 150 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_150.parquet\n",
      "Processing chunk 152/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 151 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_151.parquet\n",
      "Processing chunk 153/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 152 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_152.parquet\n",
      "Processing chunk 154/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 153 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_153.parquet\n",
      "Processing chunk 155/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 154 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_154.parquet\n",
      "Processing chunk 156/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 155 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_155.parquet\n",
      "Processing chunk 157/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 156 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_156.parquet\n",
      "Processing chunk 158/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 157 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_157.parquet\n",
      "Processing chunk 159/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 158 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_158.parquet\n",
      "Processing chunk 160/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 159 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_159.parquet\n",
      "Processing chunk 161/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 160 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_160.parquet\n",
      "Processing chunk 162/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 161 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_161.parquet\n",
      "Processing chunk 163/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 162 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_162.parquet\n",
      "Processing chunk 164/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 163 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_163.parquet\n",
      "Processing chunk 165/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 164 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_164.parquet\n",
      "Processing chunk 166/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 165 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_165.parquet\n",
      "Processing chunk 167/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 166 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_166.parquet\n",
      "Processing chunk 168/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 167 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_167.parquet\n",
      "Processing chunk 169/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 168 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_168.parquet\n",
      "Processing chunk 170/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 169 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_169.parquet\n",
      "Processing chunk 171/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 170 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_170.parquet\n",
      "Processing chunk 172/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 171 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_171.parquet\n",
      "Processing chunk 173/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 172 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_172.parquet\n",
      "Processing chunk 174/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 173 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_173.parquet\n",
      "Processing chunk 175/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 174 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_174.parquet\n",
      "Processing chunk 176/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 175 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_175.parquet\n",
      "Processing chunk 177/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 176 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_176.parquet\n",
      "Processing chunk 178/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 177 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_177.parquet\n",
      "Processing chunk 179/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 178 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_178.parquet\n",
      "Processing chunk 180/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 179 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_179.parquet\n",
      "Processing chunk 181/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 180 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_180.parquet\n",
      "Processing chunk 182/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 181 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_181.parquet\n",
      "Processing chunk 183/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 182 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_182.parquet\n",
      "Processing chunk 184/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 183 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_183.parquet\n",
      "Processing chunk 185/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 184 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_184.parquet\n",
      "Processing chunk 186/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 185 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_185.parquet\n",
      "Processing chunk 187/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 186 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_186.parquet\n",
      "Processing chunk 188/207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 187 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_187.parquet\n",
      "Processing chunk 189/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 188 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_188.parquet\n",
      "Processing chunk 190/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 189 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_189.parquet\n",
      "Processing chunk 191/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 190 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_190.parquet\n",
      "Processing chunk 192/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 191 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_191.parquet\n",
      "Processing chunk 193/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 192 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_192.parquet\n",
      "Processing chunk 194/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 193 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_193.parquet\n",
      "Processing chunk 195/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 194 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_194.parquet\n",
      "Processing chunk 196/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 195 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_195.parquet\n",
      "Processing chunk 197/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 196 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_196.parquet\n",
      "Processing chunk 198/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 197 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_197.parquet\n",
      "Processing chunk 199/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 198 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_198.parquet\n",
      "Processing chunk 200/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 199 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_199.parquet\n",
      "Processing chunk 201/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 200 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_200.parquet\n",
      "Processing chunk 202/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 201 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_201.parquet\n",
      "Processing chunk 203/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 202 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_202.parquet\n",
      "Processing chunk 204/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 203 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_203.parquet\n",
      "Processing chunk 205/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 204 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_204.parquet\n",
      "Processing chunk 206/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 205 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_205.parquet\n",
      "Processing chunk 207/207\n",
      "Type of 'genre' after merge: <class 'list'>\n",
      "Type of 'genre_2' after merge: <class 'list'>\n",
      "Chunk 206 saved to /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks/chunk_206.parquet\n",
      "Concatenating all processed chunks...\n",
      "Final DataFrame:\n",
      "        id1                               id2     score     genre  \\\n",
      "0  $5_a_day                     hotel_de_love  0.995643  [Comedy]   \n",
      "1  $5_a_day                 the_midnight_gang  0.993423  [Comedy]   \n",
      "2  $5_a_day          ce_quil_ne_faut_pas_dire  0.989711  [Comedy]   \n",
      "3  $5_a_day                        simple_men  0.986981  [Comedy]   \n",
      "4  $5_a_day  handsome_a_netflix_mystery_movie  0.986540  [Comedy]   \n",
      "\n",
      "                    genre_2  \n",
      "0         [Romance, Comedy]  \n",
      "1                 [Unknown]  \n",
      "2                 [Unknown]  \n",
      "3  [Comedy, Drama, Romance]  \n",
      "4                  [Comedy]  \n",
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "# Directory to save processed chunks\n",
    "output_dir = '/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Function to check if two movies share at least one genre\n",
    "def has_common_genre(genres1, genres2):\n",
    "\n",
    "    if 'Unknown' in genres1 or 'Unknown' in genres2:\n",
    "        return True\n",
    "\n",
    "    return not set(genres1).isdisjoint(genres2)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk, movies):\n",
    "    # Reset the index to make 'id' a column\n",
    "    chunk = chunk.reset_index()\n",
    "    \n",
    "    # Melt the matrix to transform it into pairs of IDs and similarity scores\n",
    "    chunk_melt = chunk.melt(id_vars='id', var_name='id2', value_name='score')\n",
    "    chunk_melt.rename(columns={'id': 'id1'}, inplace=True)\n",
    "    \n",
    "    # Convert columns to lighter data types to save memory\n",
    "    chunk_melt['id1'] = chunk_melt['id1'].astype('category')\n",
    "    chunk_melt['id2'] = chunk_melt['id2'].astype('category')\n",
    "    chunk_melt['score'] = chunk_melt['score'].astype('float32')\n",
    "    \n",
    "    # Inner join with the genres table to filter movies with at least one common genre\n",
    "    chunk_melt = chunk_melt.merge(movies_subset, left_on='id1', right_on='id', suffixes=('', '_1'))\n",
    "    chunk_melt = chunk_melt.merge(movies_subset, left_on='id2', right_on='id', suffixes=('', '_2'))\n",
    "    \n",
    "    # Rename gender columns\n",
    "    chunk_melt.rename(columns={'genre': 'genre', 'genre_1': 'genre_2'}, inplace=True)\n",
    "    \n",
    "    # Check types after merge\n",
    "    print(\"Type of 'genre' after merge:\", type(chunk_melt['genre'].iloc[0]))\n",
    "    print(\"Type of 'genre_2' after merge:\", type(chunk_melt['genre_2'].iloc[0]))\n",
    "    \n",
    "    # Filter out movies that don't share a common genre\n",
    "    chunk_melt = chunk_melt[chunk_melt.apply(lambda row: has_common_genre(row['genre'], row['genre_2']), axis=1)]\n",
    "   \n",
    "    # Group by id1 and keep the top 6 most similar movies\n",
    "    chunk_melt = chunk_melt.sort_values(by=['id1', 'score'], ascending=[True, False])\n",
    "    chunk_melt = chunk_melt.groupby('id1').head(6)\n",
    "    \n",
    "    chunk_melt = chunk_melt[chunk_melt['id1'] != chunk_melt['id2']]\n",
    "    \n",
    "    chunk_melt = chunk_melt.groupby('id1').head(5)\n",
    "    \n",
    "    return chunk_melt[['id1', 'id2', 'score', 'genre', 'genre_2']]\n",
    "\n",
    "# Split the DataFrame into chunks\n",
    "chunk_size = 300  \n",
    "num_chunks = int(np.ceil(sim_df.shape[0] / chunk_size))\n",
    "\n",
    "# List to store paths of processed files\n",
    "processed_files = []\n",
    "\n",
    "# Process each chunk\n",
    "for i in range(num_chunks):\n",
    "    # Check if the chunk has already been processed\n",
    "    file_path = os.path.join(output_dir, f'chunk_{i}.parquet')\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Chunk {i} already processed. Skipping...\")\n",
    "        processed_files.append(file_path)\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing chunk {i + 1}/{num_chunks}\")\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, sim_df.shape[0])\n",
    "    chunk = sim_df.iloc[start:end, :]  # Select the current chunk\n",
    "    \n",
    "    # Process the chunk\n",
    "    result_chunk = process_chunk(chunk, movies_subset)\n",
    "    \n",
    "    # Save the processed chunk to a Parquet file\n",
    "    result_chunk.to_parquet(file_path)\n",
    "    processed_files.append(file_path)\n",
    "    print(f\"Chunk {i} saved to {file_path}\")\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "print(\"Concatenating all processed chunks...\")\n",
    "df_final = pd.concat([pd.read_parquet(file, columns=['id1', 'id2', 'score', 'genre', 'genre_2']) for file in processed_files], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"Final DataFrame:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# Save the final DataFrame\n",
    "df_final.to_parquet(os.path.join(output_dir, 'df_final.parquet'))\n",
    "print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee1cb00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m size_bytes \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(df_final)\n\u001b[1;32m      3\u001b[0m size_gb \u001b[38;5;241m=\u001b[39m size_bytes \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of the matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_gb\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "size_bytes = sys.getsizeof(df_final)\n",
    "\n",
    "size_gb = size_bytes / 1e9\n",
    "\n",
    "print(f\"Size of the matrix: {size_gb:.6f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1176af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_final \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcompute()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dd' is not defined"
     ]
    }
   ],
   "source": [
    "df_final = dd.read_parquet('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a2924b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>score</th>\n",
       "      <th>genre</th>\n",
       "      <th>genre_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7426500</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>hotel_de_love</td>\n",
       "      <td>0.995643</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15930600</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>the_midnight_gang</td>\n",
       "      <td>0.993423</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668400</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>ce_quil_ne_faut_pas_dire</td>\n",
       "      <td>0.989711</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13512900</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>simple_men</td>\n",
       "      <td>0.986981</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6933300</th>\n",
       "      <td>$5_a_day</td>\n",
       "      <td>handsome_a_netflix_mystery_movie</td>\n",
       "      <td>0.986540</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309990</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>road_scholar</td>\n",
       "      <td>0.998716</td>\n",
       "      <td>[Documentary, Music]</td>\n",
       "      <td>[Documentary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309991</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>yucatan_2018</td>\n",
       "      <td>0.998606</td>\n",
       "      <td>[Documentary, Music]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309992</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>1212267-blast</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>[Documentary, Music]</td>\n",
       "      <td>[Documentary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309993</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>the_cigarette_girl_from_mosselprom</td>\n",
       "      <td>0.992749</td>\n",
       "      <td>[Documentary, Music]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309994</th>\n",
       "      <td>zz_top_that_little_ol_band_from_texas</td>\n",
       "      <td>fyre_fraud</td>\n",
       "      <td>0.991581</td>\n",
       "      <td>[Documentary, Music]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>619990 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id1  \\\n",
       "7426500                                $5_a_day   \n",
       "15930600                               $5_a_day   \n",
       "3668400                                $5_a_day   \n",
       "13512900                               $5_a_day   \n",
       "6933300                                $5_a_day   \n",
       "...                                         ...   \n",
       "309990    zz_top_that_little_ol_band_from_texas   \n",
       "309991    zz_top_that_little_ol_band_from_texas   \n",
       "309992    zz_top_that_little_ol_band_from_texas   \n",
       "309993    zz_top_that_little_ol_band_from_texas   \n",
       "309994    zz_top_that_little_ol_band_from_texas   \n",
       "\n",
       "                                         id2     score                 genre  \\\n",
       "7426500                        hotel_de_love  0.995643              [Comedy]   \n",
       "15930600                   the_midnight_gang  0.993423              [Comedy]   \n",
       "3668400             ce_quil_ne_faut_pas_dire  0.989711              [Comedy]   \n",
       "13512900                          simple_men  0.986981              [Comedy]   \n",
       "6933300     handsome_a_netflix_mystery_movie  0.986540              [Comedy]   \n",
       "...                                      ...       ...                   ...   \n",
       "309990                          road_scholar  0.998716  [Documentary, Music]   \n",
       "309991                          yucatan_2018  0.998606  [Documentary, Music]   \n",
       "309992                         1212267-blast  0.995124  [Documentary, Music]   \n",
       "309993    the_cigarette_girl_from_mosselprom  0.992749  [Documentary, Music]   \n",
       "309994                            fyre_fraud  0.991581  [Documentary, Music]   \n",
       "\n",
       "                           genre_2  \n",
       "7426500          [Romance, Comedy]  \n",
       "15930600                 [Unknown]  \n",
       "3668400                  [Unknown]  \n",
       "13512900  [Comedy, Drama, Romance]  \n",
       "6933300                   [Comedy]  \n",
       "...                            ...  \n",
       "309990               [Documentary]  \n",
       "309991                   [Unknown]  \n",
       "309992               [Documentary]  \n",
       "309993                   [Unknown]  \n",
       "309994                   [Unknown]  \n",
       "\n",
       "[619990 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_clean = df_final.drop_duplicates(subset=['id1', 'id2'])\n",
    "df_final_clean.to_csv(\"df_final_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_clean = df_final.drop_duplicates(subset=['id1', 'id2'])\n",
    "\n",
    "# Group by id1 and count unique id2 values\n",
    "group_counts = df_final_clean.groupby('id1')['id2'].nunique()\n",
    "\n",
    "# Check the distribution of the counts\n",
    "print(\"Distribution of unique recommendations per film:\")\n",
    "print(group_counts.value_counts())\n",
    "\n",
    "# List films that don't have exactly 5 unique recommendations\n",
    "films_not_five = group_counts[group_counts != 5]\n",
    "print(\"\\nFilms with a number of recommendations different from 5:\")\n",
    "print(films_not_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3564acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = dd.read_parquet('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/processed_chunks').compute()\n",
    "\n",
    "# Group by id1 and count unique id2 values\n",
    "group_counts = df_final.groupby('id1')['id2'].nunique()\n",
    "\n",
    "# Check the distribution of the counts\n",
    "print(\"Distribution of unique recommendations per film:\")\n",
    "print(group_counts.value_counts())\n",
    "\n",
    "# List films that don't have exactly 5 unique recommendations\n",
    "films_not_five = group_counts[group_counts != 5]\n",
    "print(\"\\nFilms with a number of recommendations different from 5:\")\n",
    "print(films_not_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04914c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac683952",
   "metadata": {},
   "source": [
    "# Validating the Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d311696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion                  admiration  amusement  anger  annoyance  approval  \\\n",
      "id                                                                           \n",
      "10008698-king_corn              0.0     34.124    0.0        0.0    31.708   \n",
      "1006250-dragnet                 0.0     33.403    0.0        0.0    28.819   \n",
      "1038237-platinum_blonde         0.0     43.619    0.0        0.0    26.868   \n",
      "1118698-empire                  0.0     30.693    0.0        0.0    28.538   \n",
      "1120843-hunted                  0.0     36.116    0.0        0.0    31.508   \n",
      "...                             ...        ...    ...        ...       ...   \n",
      "used_people                     0.0     36.491    0.0        0.0    30.280   \n",
      "web_junkie                      0.0     40.971    0.0        0.0    29.956   \n",
      "wheres_marlowe                  0.0     33.271    0.0        0.0    20.512   \n",
      "workforce                       0.0     32.062    0.0        0.0    26.473   \n",
      "wrong_turn_2                    0.0     31.592    0.0        0.0    30.241   \n",
      "\n",
      "emotion                  caring  confusion  curiosity  desire  disappointment  \\\n",
      "id                                                                              \n",
      "10008698-king_corn          0.0        0.0        0.0     0.0             0.0   \n",
      "1006250-dragnet             0.0        0.0        0.0     0.0             0.0   \n",
      "1038237-platinum_blonde     0.0        0.0        0.0     0.0             0.0   \n",
      "1118698-empire              0.0        0.0        0.0     0.0             0.0   \n",
      "1120843-hunted              0.0        0.0        0.0     0.0             0.0   \n",
      "...                         ...        ...        ...     ...             ...   \n",
      "used_people                 0.0        0.0        0.0     0.0             0.0   \n",
      "web_junkie                  0.0        0.0        0.0     0.0             0.0   \n",
      "wheres_marlowe              0.0        0.0        0.0     0.0             0.0   \n",
      "workforce                   0.0        0.0        0.0     0.0             0.0   \n",
      "wrong_turn_2                0.0        0.0        0.0     0.0             0.0   \n",
      "\n",
      "emotion                  ...  joy  love  nervousness  optimism  pride  \\\n",
      "id                       ...                                            \n",
      "10008698-king_corn       ...  0.0   0.0          0.0       0.0    0.0   \n",
      "1006250-dragnet          ...  0.0   0.0          0.0       0.0    0.0   \n",
      "1038237-platinum_blonde  ...  0.0   0.0          0.0       0.0    0.0   \n",
      "1118698-empire           ...  0.0   0.0          0.0       0.0    0.0   \n",
      "1120843-hunted           ...  0.0   0.0          0.0       0.0    0.0   \n",
      "...                      ...  ...   ...          ...       ...    ...   \n",
      "used_people              ...  0.0   0.0          0.0       0.0    0.0   \n",
      "web_junkie               ...  0.0   0.0          0.0       0.0    0.0   \n",
      "wheres_marlowe           ...  0.0   0.0          0.0       0.0    0.0   \n",
      "workforce                ...  0.0   0.0          0.0       0.0    0.0   \n",
      "wrong_turn_2             ...  0.0   0.0          0.0       0.0    0.0   \n",
      "\n",
      "emotion                  realization  relief  remorse  sadness  surprise  \n",
      "id                                                                        \n",
      "10008698-king_corn               0.0     0.0      0.0      0.0    34.168  \n",
      "1006250-dragnet                  0.0     0.0      0.0      0.0    37.778  \n",
      "1038237-platinum_blonde          0.0     0.0      0.0      0.0    29.513  \n",
      "1118698-empire                   0.0     0.0      0.0      0.0    40.769  \n",
      "1120843-hunted                   0.0     0.0      0.0      0.0    32.375  \n",
      "...                              ...     ...      ...      ...       ...  \n",
      "used_people                      0.0     0.0      0.0      0.0    33.229  \n",
      "web_junkie                       0.0     0.0      0.0      0.0    29.073  \n",
      "wheres_marlowe                   0.0     0.0      0.0      0.0    46.218  \n",
      "workforce                        0.0     0.0      0.0      0.0    41.465  \n",
      "wrong_turn_2                     0.0     0.0      0.0      0.0    38.166  \n",
      "\n",
      "[108 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "colunas = ['approval', 'amusement', 'surprise']\n",
    "\n",
    "condicao = (reviews_wide[colunas] > 20)\n",
    "\n",
    "linhas_filtradas = reviews_wide[condicao.sum(axis=1) >= 3]\n",
    "\n",
    "print(linhas_filtradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0fc6cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_wide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Look at the matrix and compare the similarity between films you already know \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# are similar or different based on their emotion percentage profiles.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# If two films have very similar emotion distributions, the similarity score should be high.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Films with similar emotion percentages\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m film_a \u001b[38;5;241m=\u001b[39m reviews_wide\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrong_turn_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m film_b \u001b[38;5;241m=\u001b[39m reviews_wide\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10008698-king_corn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check their similarity in the matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews_wide' is not defined"
     ]
    }
   ],
   "source": [
    "# Look at the matrix and compare the similarity between films you already know \n",
    "# are similar or different based on their emotion percentage profiles.\n",
    "# If two films have very similar emotion distributions, the similarity score should be high.\n",
    "\n",
    "# Films with similar emotion percentages\n",
    "film_a = reviews_wide.loc['wrong_turn_2']\n",
    "film_b = reviews_wide.loc['10008698-king_corn']\n",
    "\n",
    "# Check their similarity in the matrix\n",
    "similarity = sim_df.loc['wrong_turn_2', '10008698-king_corn']\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e883878",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_wide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Films with different emotion percentages\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m film_a \u001b[38;5;241m=\u001b[39m reviews_wide\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkatherine_ryan_glitter_room\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m film_b \u001b[38;5;241m=\u001b[39m reviews_wide\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m009_re_cyborg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check their similarity in the matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews_wide' is not defined"
     ]
    }
   ],
   "source": [
    "# Films with different emotion percentages\n",
    "film_a = reviews_wide.loc['katherine_ryan_glitter_room']\n",
    "film_b = reviews_wide.loc['009_re_cyborg']\n",
    "\n",
    "# Check their similarity in the matrix\n",
    "similarity = sim_df.loc['katherine_ryan_glitter_room', '009_re_cyborg']\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb7034d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply PCA to reduce dimensionality\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m)  \u001b[38;5;66;03m# Retain 97% of variance\u001b[39;00m\n\u001b[1;32m      3\u001b[0m reviews_reduced \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(reviews_wide\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Choose two films to compare\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=0.97)  # Retain 97% of variance\n",
    "reviews_reduced = pca.fit_transform(reviews_wide.values)\n",
    "\n",
    "# Choose two films to compare\n",
    "film1_id = 'katherine_ryan_glitter_room'  # Replace with the first film's ID\n",
    "film2_id = '009_re_cyborg'               # Replace with the second film's ID\n",
    "\n",
    "# Find the films' indices in the reviews_wide matrix\n",
    "film1_idx = reviews_wide.index.get_loc(film1_id)\n",
    "film2_idx = reviews_wide.index.get_loc(film2_id)\n",
    "\n",
    "# Select the film vectors after PCA\n",
    "film1_vector = reviews_reduced[film1_idx].reshape(1, -1)  # Shape into (1, n_features)\n",
    "film2_vector = reviews_reduced[film2_idx].reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity manually\n",
    "manual_similarity = cosine_similarity(film1_vector, film2_vector)[0][0]\n",
    "\n",
    "# Retrieve the corresponding value from the sim_df matrix\n",
    "matrix_similarity = sim_df.loc[film1_id, film2_id]\n",
    "\n",
    "# Display results\n",
    "print(f\"Manual similarity between {film1_id} and {film2_id}: {manual_similarity}\")\n",
    "print(f\"Similarity in sim_df: {matrix_similarity}\")\n",
    "\n",
    "# Check if the values match (or are very close, due to rounding)\n",
    "if np.isclose(manual_similarity, matrix_similarity, atol=1e-6):\n",
    "    print(\"Values are identical (within a small tolerance).\")\n",
    "else:\n",
    "    print(\"Values differ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724ad0c",
   "metadata": {},
   "source": [
    "# Main Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3c46077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_movies(sim_df, movies, reviews, top_n=5):\n",
    "    movies = movies.copy()\n",
    "\n",
    "    if isinstance(movies['genre'].iloc[0], str):\n",
    "        movies['genre'] = movies['genre'].apply(ast.literal_eval)\n",
    "\n",
    "    while True:\n",
    "        # Ask the user for their favorite movie\n",
    "        favorite_movie = input(\"Enter the name of your favorite movie (or type 'exit' to quit): \").strip()\n",
    "\n",
    "        # Allow the user to exit\n",
    "        if favorite_movie.lower() == 'exit':\n",
    "            print(\"Exiting the recommendation system. Goodbye!\")\n",
    "            return None\n",
    "\n",
    "        # Normalize the movie name\n",
    "        favorite_movie_normalized = normalize_name(favorite_movie)\n",
    "\n",
    "        # Find the movie in the dataset\n",
    "        matching_movies = movies[movies['title_normalized'] == favorite_movie_normalized]\n",
    "        if matching_movies.empty:\n",
    "            print(f\"The movie '{favorite_movie}' was not found. Please check the name and try again.\")\n",
    "            continue  \n",
    "        else:\n",
    "            favorite_movie_id = matching_movies.iloc[0]['id']\n",
    "            favorite_movie_title = matching_movies.iloc[0]['title']\n",
    "            print(f\"Found movie: {favorite_movie_title} (ID: {favorite_movie_id})\")\n",
    "            break  \n",
    "\n",
    "    # Check if the movie is in the similarity matrix\n",
    "    if favorite_movie_id not in sim_df.columns:\n",
    "        print(f\"Movie ID '{favorite_movie_id}' not found in the similarity matrix.\")\n",
    "        return None\n",
    "\n",
    "    # Get similarity scores and sort by highest similarity\n",
    "    movie_similarities = sim_df[favorite_movie_id].sort_values(ascending=False)\n",
    "\n",
    "    # Remove the movie itself from recommendations\n",
    "    movie_similarities = movie_similarities.drop(favorite_movie_id, errors='ignore')\n",
    "\n",
    "    # Get genres of the favorite movie\n",
    "    favorite_movie_genres = set(matching_movies['genre'].explode().values)\n",
    "    print(favorite_movie_genres)\n",
    "\n",
    "    # Filter recommendations by shared genre\n",
    "    recommended_ids = movies[movies['genre'].apply(lambda genres: any(genre in favorite_movie_genres for genre in genres))]['id'].unique()\n",
    "    movie_similarities = movie_similarities[movie_similarities.index.isin(recommended_ids)]\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_recommendations = movie_similarities.head(top_n).reset_index()\n",
    "    top_recommendations.columns = ['id', 'similarity']\n",
    "\n",
    "    # Merge with movie details\n",
    "    recommended_movies = top_recommendations.merge(movies, on='id', how='left')\n",
    "\n",
    "    # Select relevant columns\n",
    "    result = recommended_movies[['id', 'title', 'director', 'originalLanguage', 'runtimeMinutes', \n",
    "                                 'genre', 'release_year', 'tomatoMeter', 'audienceScore', \n",
    "                                 'similarity', 'emotions']]\n",
    "\n",
    "    # Display emotional profile of favorite movie\n",
    "    favorite_movie_emotions = matching_movies.iloc[0]['emotions']\n",
    "    print(f\"Emotional profile of '{favorite_movie_title}':\")  \n",
    "    if isinstance(favorite_movie_emotions, list):\n",
    "        print(f\"   ❤️ Emotions: {', '.join([f'{mood} ({percentage:.1f}%)' for mood, percentage in favorite_movie_emotions])}\")\n",
    "    else:\n",
    "        print(f\"   ❤️ Emotions: {favorite_movie_emotions}\")  \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    \n",
    "    # Print formatted recommendations\n",
    "    print(f\"\\nTop {top_n} movie recommendations based on '{favorite_movie_title}':\\n\")\n",
    "\n",
    "\n",
    "    # Display recommendations\n",
    "    for _, row in result.iterrows():\n",
    "        print(f\"🎬 Movie: {row['title']}\")\n",
    "        print(f\"   🎬 Director: {row['director']}\")\n",
    "        print(f\"   🌍 Language: {row['originalLanguage']}\")\n",
    "        print(f\"   ⏳ Duration: {row['runtimeMinutes']} min\")\n",
    "        print(f\"   🎭 Genre: {', '.join(row['genre'])}\")\n",
    "        print(f\"   📅 Year: {row['release_year']}\")\n",
    "        print(f\"   🍅 Tomatometer: {row['tomatoMeter']}%\")\n",
    "        print(f\"   🎟️ Audience Score: {row['audienceScore']}%\")\n",
    "        print(f\"   🔗 Similarity Score: {row['similarity']:.5f}\")\n",
    "        if isinstance(row['emotions'], list):\n",
    "            print(f\"   ❤️ Emotions: {', '.join([f'{mood} ({percentage:.1f}%)' for mood, percentage in row['emotions']])}\")\n",
    "        else:\n",
    "            print(f\"   ❤️ Emotions: {row['emotions']}\")  \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da28f6",
   "metadata": {},
   "source": [
    "# Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eace65a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of your favorite movie (or type 'exit' to quit): five dollars a day\n",
      "Found movie: Five Dollars a Day (ID: $5_a_day)\n",
      "{'Comedy'}\n",
      "Emotional profile of 'Five Dollars a Day':\n",
      "   ❤️ Emotions: [('disapproval', 38.70600543243529), ('admiration', 33.1336321492672), ('annoyance', 28.16036241829752)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 movie recommendations based on 'Five Dollars a Day':\n",
      "\n",
      "🎬 Movie: Hotel de Love\n",
      "   🎬 Director: Craig Rosenberg\n",
      "   🌍 Language: English\n",
      "   ⏳ Duration: 95.0 min\n",
      "   🎭 Genre: Romance, Comedy\n",
      "   📅 Year: Unknown\n",
      "   🍅 Tomatometer: 22.0%\n",
      "   🎟️ Audience Score: 54.0%\n",
      "   🔗 Similarity Score: 0.99564\n",
      "   ❤️ Emotions: [('disapproval', 37.67944043788483), ('admiration', 36.74494103061423), ('annoyance', 25.575618531500933)]\n",
      "--------------------------------------------------\n",
      "🎬 Movie: Simple Men\n",
      "   🎬 Director: Hal Hartley\n",
      "   🌍 Language: English\n",
      "   ⏳ Duration: 105.0 min\n",
      "   🎭 Genre: Comedy, Drama, Romance\n",
      "   📅 Year: 2004\n",
      "   🍅 Tomatometer: 91.0%\n",
      "   🎟️ Audience Score: 89.0%\n",
      "   🔗 Similarity Score: 0.98698\n",
      "   ❤️ Emotions: [('disapproval', 44.171436068251964), ('admiration', 33.65403737437839), ('annoyance', 22.174526557369653)]\n",
      "--------------------------------------------------\n",
      "🎬 Movie: Handsome: A Netflix Mystery Movie\n",
      "   🎬 Director: Jeff Garlin\n",
      "   🌍 Language: English\n",
      "   ⏳ Duration: 103.0 min\n",
      "   🎭 Genre: Comedy\n",
      "   📅 Year: 2017\n",
      "   🍅 Tomatometer: 65.76%\n",
      "   🎟️ Audience Score: 31.0%\n",
      "   🔗 Similarity Score: 0.98654\n",
      "   ❤️ Emotions: [('disapproval', 45.653808792131116), ('admiration', 30.554937861873288), ('annoyance', 23.791253345995585)]\n",
      "--------------------------------------------------\n",
      "🎬 Movie: I Love Melvin\n",
      "   🎬 Director: Don Weis\n",
      "   🌍 Language: English\n",
      "   ⏳ Duration: 76.0 min\n",
      "   🎭 Genre: Musical, Comedy\n",
      "   📅 Year: 1953\n",
      "   🍅 Tomatometer: 65.76%\n",
      "   🎟️ Audience Score: 58.0%\n",
      "   🔗 Similarity Score: 0.98644\n",
      "   ❤️ Emotions: [('admiration', 38.62660210233127), ('disapproval', 32.85559726258748), ('annoyance', 28.517800635081255)]\n",
      "--------------------------------------------------\n",
      "🎬 Movie: Everything Under Control\n",
      "   🎬 Director: Unknown\n",
      "   🌍 Language: Chinese\n",
      "   ⏳ Duration: 93.75 min\n",
      "   🎭 Genre: Comedy, Drama, Action, Horror\n",
      "   📅 Year: 2023\n",
      "   🍅 Tomatometer: 65.76%\n",
      "   🎟️ Audience Score: 60.0%\n",
      "   🔗 Similarity Score: 0.97544\n",
      "   ❤️ Emotions: [('admiration', 39.48153287055353), ('disapproval', 30.333672560212143), ('annoyance', 30.18479456923434)]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "recommendations = recommend_similar_movies(sim_df, movies, reviews, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ec6a416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>originalLanguage</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genre</th>\n",
       "      <th>release_year</th>\n",
       "      <th>tomatoMeter</th>\n",
       "      <th>audienceScore</th>\n",
       "      <th>similarity</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hotel_de_love</td>\n",
       "      <td>Hotel de Love</td>\n",
       "      <td>Craig Rosenberg</td>\n",
       "      <td>English</td>\n",
       "      <td>95.00</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>22.00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.995643</td>\n",
       "      <td>[('disapproval', 37.67944043788483), ('admiration', 36.74494103061423), ('annoyance', 25.575618531500933)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_men</td>\n",
       "      <td>Simple Men</td>\n",
       "      <td>Hal Hartley</td>\n",
       "      <td>English</td>\n",
       "      <td>105.00</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>2004</td>\n",
       "      <td>91.00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.986981</td>\n",
       "      <td>[('disapproval', 44.171436068251964), ('admiration', 33.65403737437839), ('annoyance', 22.174526557369653)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>handsome_a_netflix_mystery_movie</td>\n",
       "      <td>Handsome: A Netflix Mystery Movie</td>\n",
       "      <td>Jeff Garlin</td>\n",
       "      <td>English</td>\n",
       "      <td>103.00</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>2017</td>\n",
       "      <td>65.76</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.986540</td>\n",
       "      <td>[('disapproval', 45.653808792131116), ('admiration', 30.554937861873288), ('annoyance', 23.791253345995585)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i_love_melvin</td>\n",
       "      <td>I Love Melvin</td>\n",
       "      <td>Don Weis</td>\n",
       "      <td>English</td>\n",
       "      <td>76.00</td>\n",
       "      <td>[Musical, Comedy]</td>\n",
       "      <td>1953</td>\n",
       "      <td>65.76</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.986439</td>\n",
       "      <td>[('admiration', 38.62660210233127), ('disapproval', 32.85559726258748), ('annoyance', 28.517800635081255)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>everything_under_control</td>\n",
       "      <td>Everything Under Control</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>93.75</td>\n",
       "      <td>[Comedy, Drama, Action, Horror]</td>\n",
       "      <td>2023</td>\n",
       "      <td>65.76</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.975437</td>\n",
       "      <td>[('admiration', 39.48153287055353), ('disapproval', 30.333672560212143), ('annoyance', 30.18479456923434)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                              title  \\\n",
       "0                     hotel_de_love                      Hotel de Love   \n",
       "1                        simple_men                         Simple Men   \n",
       "2  handsome_a_netflix_mystery_movie  Handsome: A Netflix Mystery Movie   \n",
       "3                     i_love_melvin                      I Love Melvin   \n",
       "4          everything_under_control           Everything Under Control   \n",
       "\n",
       "          director originalLanguage  runtimeMinutes  \\\n",
       "0  Craig Rosenberg          English           95.00   \n",
       "1      Hal Hartley          English          105.00   \n",
       "2      Jeff Garlin          English          103.00   \n",
       "3         Don Weis          English           76.00   \n",
       "4          Unknown          Chinese           93.75   \n",
       "\n",
       "                             genre release_year  tomatoMeter  audienceScore  \\\n",
       "0                [Romance, Comedy]      Unknown        22.00           54.0   \n",
       "1         [Comedy, Drama, Romance]         2004        91.00           89.0   \n",
       "2                         [Comedy]         2017        65.76           31.0   \n",
       "3                [Musical, Comedy]         1953        65.76           58.0   \n",
       "4  [Comedy, Drama, Action, Horror]         2023        65.76           60.0   \n",
       "\n",
       "   similarity  \\\n",
       "0    0.995643   \n",
       "1    0.986981   \n",
       "2    0.986540   \n",
       "3    0.986439   \n",
       "4    0.975437   \n",
       "\n",
       "                                                                                                       emotions  \n",
       "0    [('disapproval', 37.67944043788483), ('admiration', 36.74494103061423), ('annoyance', 25.575618531500933)]  \n",
       "1   [('disapproval', 44.171436068251964), ('admiration', 33.65403737437839), ('annoyance', 22.174526557369653)]  \n",
       "2  [('disapproval', 45.653808792131116), ('admiration', 30.554937861873288), ('annoyance', 23.791253345995585)]  \n",
       "3    [('admiration', 38.62660210233127), ('disapproval', 32.85559726258748), ('annoyance', 28.517800635081255)]  \n",
       "4    [('admiration', 39.48153287055353), ('disapproval', 30.333672560212143), ('annoyance', 30.18479456923434)]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e24e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
