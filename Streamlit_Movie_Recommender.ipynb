{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9787c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 08:38:45.719 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-03-13 08:38:45.855 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/danielebelmiro/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-13 08:38:45.856 No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/df_final.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/parquet/core.py:543\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     blocksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mread_metadata(\n\u001b[1;32m    544\u001b[0m     fs,\n\u001b[1;32m    545\u001b[0m     paths,\n\u001b[1;32m    546\u001b[0m     categories\u001b[38;5;241m=\u001b[39mcategories,\n\u001b[1;32m    547\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    548\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    549\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    550\u001b[0m     gather_statistics\u001b[38;5;241m=\u001b[39mcalculate_divisions,\n\u001b[1;32m    551\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    552\u001b[0m     split_row_groups\u001b[38;5;241m=\u001b[39msplit_row_groups,\n\u001b[1;32m    553\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[1;32m    554\u001b[0m     aggregate_files\u001b[38;5;241m=\u001b[39maggregate_files,\n\u001b[1;32m    555\u001b[0m     ignore_metadata_file\u001b[38;5;241m=\u001b[39mignore_metadata_file,\n\u001b[1;32m    556\u001b[0m     metadata_task_size\u001b[38;5;241m=\u001b[39mmetadata_task_size,\n\u001b[1;32m    557\u001b[0m     parquet_file_extension\u001b[38;5;241m=\u001b[39mparquet_file_extension,\n\u001b[1;32m    558\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset_options,\n\u001b[1;32m    559\u001b[0m     read\u001b[38;5;241m=\u001b[39mread_options,\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_options,\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:534\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_dataset_info(\n\u001b[1;32m    535\u001b[0m     paths,\n\u001b[1;32m    536\u001b[0m     fs,\n\u001b[1;32m    537\u001b[0m     categories,\n\u001b[1;32m    538\u001b[0m     index,\n\u001b[1;32m    539\u001b[0m     gather_statistics,\n\u001b[1;32m    540\u001b[0m     filters,\n\u001b[1;32m    541\u001b[0m     split_row_groups,\n\u001b[1;32m    542\u001b[0m     blocksize,\n\u001b[1;32m    543\u001b[0m     aggregate_files,\n\u001b[1;32m    544\u001b[0m     ignore_metadata_file,\n\u001b[1;32m    545\u001b[0m     metadata_task_size,\n\u001b[1;32m    546\u001b[0m     parquet_file_extension,\n\u001b[1;32m    547\u001b[0m     kwargs,\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:1049\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     ds \u001b[38;5;241m=\u001b[39m pa_ds\u001b[38;5;241m.\u001b[39mdataset(\n\u001b[1;32m   1050\u001b[0m         paths,\n\u001b[1;32m   1051\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39m_wrapped_fs(fs),\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_processed_dataset_kwargs,\n\u001b[1;32m   1053\u001b[0m     )\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/dataset.py:765\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/dataset.py:443\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 443\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/dataset.py:362\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/df_final.parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 165\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Run the application\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m st\u001b[38;5;241m.\u001b[39mmarkdown(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<h1 class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>🎬 Movie Recommendation System</h1>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m     unsafe_allow_html\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m movies, reviews, df_final \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# User input for recommendations\u001b[39;00m\n\u001b[1;32m    157\u001b[0m favorite_movie \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtext_input(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the name of your favorite movie:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:168\u001b[0m, in \u001b[0;36mmake_cached_func_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(info\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:197\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m spinner(message, _cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:224\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:280\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mcached_message_replay_ctx\u001b[38;5;241m.\u001b[39mcalling_cached_function(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mallow_widgets\n\u001b[1;32m    279\u001b[0m ):\n\u001b[0;32m--> 280\u001b[0m     computed_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mcached_message_replay_ctx\u001b[38;5;241m.\u001b[39m_most_recent_messages\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m movies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/movies_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Carregar a matriz reduzida\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m df_final \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_final.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcompute()  \u001b[38;5;66;03m# Convertendo para pandas para facilitar a validação\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m movies, reviews, df_final\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/df_final.parquet"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to normalize names\n",
    "def normalize_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s]', '', name)  # Remove special characters\n",
    "    name = name.lower().strip()  # Convert to lowercase and remove extra spaces\n",
    "    return name\n",
    "\n",
    "# Function to generate the Rotten Tomatoes URL based on the movie ID\n",
    "def generate_rotten_tomatoes_url(movie_id):\n",
    "    return f\"https://www.rottentomatoes.com/m/{movie_id}\"\n",
    "\n",
    "# Function to extract the movie poster URL\n",
    "def get_movie_poster_url(movie_id):\n",
    "    url = f\"https://www.rottentomatoes.com/m/{movie_id}/pictures\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    poster_div = soup.find('div', {'class': 'movie_poster'})\n",
    "    if poster_div:\n",
    "        img_tag = poster_div.find('img')\n",
    "        if img_tag:\n",
    "            return img_tag['src']\n",
    "    return None\n",
    "\n",
    "# Function to load and display movie posters\n",
    "def display_poster(poster_url):\n",
    "    if poster_url:\n",
    "        try:\n",
    "            response = requests.get(poster_url)\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                st.image(img, width=150)  # Display the image\n",
    "            else:\n",
    "                st.write(\"Poster not available\")  # If the URL is invalid\n",
    "        except:\n",
    "            st.write(\"Poster not available\")  # If there's an error loading the image\n",
    "    else:\n",
    "        st.write(\"Poster not available\")  # If the URL is missing\n",
    "\n",
    "# Load data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    # Load DataFrames\n",
    "    reviews = pd.read_csv('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/reviews_emotions.csv')\n",
    "    movies = pd.read_csv('/Users/danielebelmiro/Data_Analytics_Bootcamp/Rotten/movies_final.csv')\n",
    "\n",
    "    # Carregar a matriz reduzida\n",
    "    df_final = dd.read_parquet('df_final.parquet').compute()  # Convertendo para pandas para facilitar a validação\n",
    "    \n",
    "    return movies, reviews, df_final\n",
    "\n",
    "# Recommendation function\n",
    "def recommend_similar_movies(df_final, movies, reviews, favorite_movie, top_n=5):\n",
    "    movies = movies.copy()\n",
    "\n",
    "    # Normalize the movie name\n",
    "    favorite_movie_normalized = normalize_name(favorite_movie)\n",
    "\n",
    "    # Find the movie in the dataset\n",
    "    matching_movies = movies[movies['title_normalized'] == favorite_movie_normalized]\n",
    "    if matching_movies.empty:\n",
    "        st.error(f\"The movie '{favorite_movie}' was not found. Please check the name and try again.\")\n",
    "        return None\n",
    "\n",
    "    favorite_movie_id = matching_movies.iloc[0]['id']\n",
    "    favorite_movie_title = matching_movies.iloc[0]['title']\n",
    "    st.success(f\"Movie found: {favorite_movie_title} (ID: {favorite_movie_id})\")\n",
    "\n",
    "    # Get similarity scores and sort by highest similarity\n",
    "    movie_similarities = df_final[df_final['id1'] == favorite_movie_id][['id2', 'score']]\n",
    "    movie_similarities = movie_similarities.sort_values(by='score', ascending=False)\n",
    "\n",
    "    # Top N recommendations\n",
    "    top_recommendations = movie_similarities.head(top_n).reset_index(drop=True)\n",
    "\n",
    "    # Merge with movie details\n",
    "    recommended_movies = top_recommendations.merge(movies, left_on='id2', right_on='id', how='left')\n",
    "\n",
    "    # Display the emotional profile of the favorite movie\n",
    "    favorite_movie_emotions = matching_movies.iloc[0]['emotions']\n",
    "    st.subheader(f\"Emotional profile of '{favorite_movie_title}':\")\n",
    "    if isinstance(favorite_movie_emotions, list):\n",
    "        st.write(f\"   ❤️ Emotions: {', '.join([f'{mood} ({percentage:.1f}%)' for mood, percentage in favorite_movie_emotions])}\")\n",
    "    else:\n",
    "        st.write(f\"   ❤️ Emotions: {favorite_movie_emotions}\")\n",
    "\n",
    "    # Display recommendations\n",
    "    st.subheader(f\"Top {top_n} recommendations based on '{favorite_movie_title}':\")\n",
    "    for _, row in recommended_movies.iterrows():\n",
    "        col1, col2 = st.columns([1, 3])  # Split into two columns for poster and details\n",
    "        with col1:\n",
    "            # Extract and display the movie poster\n",
    "            poster_url = get_movie_poster_url(row['id'])\n",
    "            display_poster(poster_url)\n",
    "        with col2:\n",
    "            st.write(f\"🎬 **Movie:** {row['title']}\")\n",
    "            st.write(f\"   🎬 **Director:** {row['director']}\")\n",
    "            st.write(f\"   🌍 **Language:** {row['originalLanguage']}\")\n",
    "            st.write(f\"   ⏳ **Duration:** {row['runtimeMinutes']} min\")\n",
    "            st.write(f\"   🎭 **Genre:** {', '.join(row['genre'])}\")\n",
    "            st.write(f\"   📅 **Year:** {row['release_year']}\")\n",
    "            st.write(f\"   🍅 **Tomatometer:** {row['tomatoMeter']}%\")\n",
    "            st.write(f\"   🎟️ **Audience Score:** {row['audienceScore']}%\")\n",
    "            st.write(f\"   🔗 **Similarity Score:** {row['score']:.5f}\")\n",
    "            if isinstance(row['emotions'], list):\n",
    "                st.write(f\"   ❤️ **Emotions:** {', '.join([f'{mood} ({percentage:.1f}%)' for mood, percentage in row['emotions']])}\")\n",
    "            else:\n",
    "                st.write(f\"   ❤️ **Emotions:** {row['emotions']}\")\n",
    "            # Add link to Rotten Tomatoes using the movie ID\n",
    "            rotten_tomatoes_url = generate_rotten_tomatoes_url(row['id'])\n",
    "            st.write(f\"   🍅 [Link to Rotten Tomatoes]({rotten_tomatoes_url})\")\n",
    "        \n",
    "        st.write(\"-\" * 50)\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "# Streamlit interface\n",
    "def main():\n",
    "    # Custom CSS for the title\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        .title {\n",
    "            font-size: 50px;\n",
    "            text-align: center;\n",
    "            color: #FF4B4B;\n",
    "            font-family: 'Arial', sans-serif;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # Apply the CSS class to the title\n",
    "    st.markdown(\n",
    "        '<h1 class=\"title\">🎬 Movie Recommendation System</h1>',\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    movies, reviews, df_final = load_data()\n",
    "\n",
    "\n",
    "    # User input for recommendations\n",
    "    favorite_movie = st.text_input(\"Enter the name of your favorite movie:\")\n",
    "    top_n = st.slider(\"How many recommendations do you want?\", 1, 5, 3)\n",
    "\n",
    "    if favorite_movie:\n",
    "        recommend_similar_movies(df_final, movies, reviews, favorite_movie, top_n)\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea41779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
